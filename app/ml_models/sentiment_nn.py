import re
import numpy as np
from typing import Dict, List, Tuple
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback
import time

class TrainingProgressCallback(Callback):
    """Callback para monitorear el progreso del entrenamiento"""
    def __init__(self):
        self.start_time = None
        self.epoch_times = []
        import sys
        self.stdout = sys.stdout
    
    def _print_and_flush(self, message):
        """Imprimir y hacer flush inmediatamente"""
        print(message)
        self.stdout.flush()
    
    def on_train_begin(self, logs=None):
        self.start_time = time.time()
        self._print_and_flush(f"‚è±Ô∏è [DEBUG] Entrenamiento comenz√≥ a las {time.strftime('%H:%M:%S')}")
        self._print_and_flush(f"üîç [DEBUG] Callback on_train_begin ejecutado correctamente")
    
    def on_epoch_begin(self, epoch, logs=None):
        epoch_start = time.time()
        self._print_and_flush(f"üîÑ [DEBUG] √âpoca {epoch + 1} comenzando a las {time.strftime('%H:%M:%S')}...")
        self.current_epoch_start = epoch_start
    
    def on_batch_begin(self, batch, logs=None):
        # Log cada 5 batches para no saturar, pero ver progreso
        if batch % 5 == 0 or batch == 0:
            self._print_and_flush(f"üîç [DEBUG] Batch {batch} comenzando...")
    
    def on_batch_end(self, batch, logs=None):
        # Log cada 5 batches para no saturar, pero ver progreso
        if batch % 5 == 0 or batch == 0:
            loss = logs.get('loss', 'N/A')
            acc = logs.get('accuracy', 'N/A')
            self._print_and_flush(f"üîç [DEBUG] Batch {batch} completado - loss: {loss}, accuracy: {acc}")
    
    def on_epoch_end(self, epoch, logs=None):
        epoch_time = time.time() - self.current_epoch_start
        self.epoch_times.append(epoch_time)
        loss = logs.get('loss', 'N/A')
        accuracy = logs.get('accuracy', 'N/A')
        val_loss = logs.get('val_loss', 'N/A')
        val_accuracy = logs.get('val_accuracy', 'N/A')
        self._print_and_flush(f"‚úÖ [DEBUG] √âpoca {epoch + 1} completada en {epoch_time:.2f}s - loss: {loss:.4f}, accuracy: {accuracy:.4f}, val_loss: {val_loss}, val_accuracy: {val_accuracy}")
    
    def on_train_end(self, logs=None):
        total_time = time.time() - self.start_time
        self._print_and_flush(f"‚è±Ô∏è [DEBUG] Entrenamiento terminado en {total_time:.2f}s total")
        if self.epoch_times:
            avg_time = sum(self.epoch_times) / len(self.epoch_times)
            self._print_and_flush(f"üìä [DEBUG] Tiempo promedio por √©poca: {avg_time:.2f}s")
        else:
            self._print_and_flush(f"‚ö†Ô∏è [DEBUG] No se registraron √©pocas completadas")

class SentimentNeuralNetwork:
    def __init__(self, max_words=5000, max_len=100):
        # Red neuronal LSTM basada en texto - Optimizado para p√°rrafos largos
        # max_words: 5000 (vocabulario amplio para mejor comprensi√≥n)
        # max_len: 100 (longitud suficiente para p√°rrafos completos)
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
        self.label_encoder = LabelEncoder()
        self.model = None
        self.is_trained = False
        
    def clean_text(self, text: str) -> str:
        """
        Limpieza de texto mejorada con normalizaci√≥n y correcci√≥n de encoding.
        
        ‚ö†Ô∏è IMPORTANTE: Este m√©todo SOLO limpia el texto (corrige encoding, normaliza).
        NO clasifica sentimientos. La clasificaci√≥n se hace 100% por la red neuronal LSTM.
        
        El diccionario 'encoding_fixes' es SOLO para corregir problemas de encoding
        de archivos CSV/Excel (ej: √É¬© -> √©). NO es un diccionario de sentimientos.
        """
        if not text:
            return ""
        
        # ‚ö†Ô∏è SOLO CORRECCI√ìN DE ENCODING - NO CLASIFICACI√ìN DE SENTIMIENTOS
        # Esto corrige problemas cuando Excel guarda UTF-8 pero se lee como Latin-1
        # Ejemplo: "√É¬©" (mal codificado) -> "√©" (correcto)
        # Esto NO afecta la clasificaci√≥n de sentimientos, solo limpia el texto
        encoding_fixes = {
            # Caracteres mal codificados m√°s comunes (UTF-8 mal le√≠do como Latin-1)
            '√É¬°': '√°', '√É¬©': '√©', '√É¬≠': '√≠', '√É¬≥': '√≥', '√É¬∫': '√∫',
            '√É¬±': '√±', '√É¬º': '√º', 
            '√É': '√Å', '√É‚Ä∞': '√â', '√É': '√ç', '√É"': '√ì', '√É≈°': '√ö',
            '√É': '√ë', '√É≈ì': '√ú',
            # Caracteres raros que aparecen en Excel
            '√¢‚Ç¨‚Ñ¢': "'", '√¢‚Ç¨≈ì': '"', '√¢‚Ç¨': '"', '√¢‚Ç¨"': '‚Äî', '√¢‚Ç¨"': '‚Äì',
            # Limpiar caracteres de control
            '\ufeff': '',  # BOM de UTF-8
            '\x00': '',  # Null bytes
        }
        
        # Aplicar correcciones de encoding (SOLO limpieza, NO clasificaci√≥n)
        for wrong, correct in encoding_fixes.items():
            text = text.replace(wrong, correct)
        
        # Intentar correcci√≥n autom√°tica de encoding si detectamos problemas
        if '√É' in text:
            try:
                # Intentar decodificar como Latin-1 y recodificar como UTF-8
                text = text.encode('latin-1', errors='ignore').decode('utf-8', errors='ignore')
            except:
                pass
        
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Normalizar tildes y caracteres especiales
        # IMPORTANTE: El modelo fue entrenado eliminando tildes para normalizar
        # "atenci√≥n" y "atencion" se tratan igual, lo cual es √∫til para el modelo
        replacements = {
            '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
            '√±': 'n', '√º': 'u'
        }
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # Eliminar URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Eliminar menciones y hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        
        # Eliminar caracteres especiales excepto letras, n√∫meros y espacios
        text = re.sub(r'[^a-z0-9\s]', ' ', text)
        
        # Eliminar espacios m√∫ltiples
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _is_valid_comment(self, comment: str) -> bool:
        """
        Valida si un comentario tiene sentido y debe incluirse en el dataset.
        Filtra comentarios sin sentido (solo n√∫meros, solo s√≠mbolos, muy cortos, etc.)
        """
        if not comment or len(comment.strip()) < 3:
            return False
        
        # Limpiar para validaci√≥n
        cleaned = comment.lower().strip()
        
        # Verificar que tenga al menos una letra
        has_letter = any(c.isalpha() for c in cleaned)
        if not has_letter:
            return False
        
        # Verificar que tenga al menos una palabra v√°lida (no solo n√∫meros/s√≠mbolos)
        words = cleaned.split()
        valid_words = [w for w in words if any(c.isalpha() for c in w)]
        if len(valid_words) < 1:
            return False
        
        # Verificar que no sea solo n√∫meros y s√≠mbolos
        if all(c.isdigit() or c in '.,;:!?-/()' or c.isspace() for c in cleaned):
            return False
        
        # Verificar que tenga sentido (al menos 1 palabra significativa o 2 palabras)
        meaningful_words = [w for w in valid_words if len(w) >= 3 or w in ['no', 'si', 'ya', 'el', 'la', 'un', 'una', 'me', 'le', 'se']]
        if len(meaningful_words) < 1 and len(valid_words) < 2:
            return False
        
        return True
    
    def _normalize_for_comparison(self, text: str) -> str:
        """
        Normaliza texto para comparaci√≥n y evitar repeticiones exactas.
        Elimina tildes y n√∫meros para comparar solo la estructura sem√°ntica.
        """
        if not text:
            return ""
        
        # Normalizar tildes
        replacements = {
            '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
            '√±': 'n', '√º': 'u'
        }
        text = text.lower()
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # Remover n√∫meros para comparaci√≥n (solo estructura sem√°ntica)
        text = re.sub(r'\d+', '', text)
        
        # Limpiar espacios
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def prepare_data(self, texts: List[str], labels: List[str] = None) -> Tuple:
        """
        Preparar datos para entrenamiento o predicci√≥n.
        
        ‚ö†Ô∏è IMPORTANTE: Este m√©todo SOLO convierte texto a n√∫meros.
        NO clasifica sentimientos. La clasificaci√≥n se hace 100% por la red neuronal LSTM.
        
        Flujo:
        1. Limpia el texto (encoding, normalizaci√≥n)
        2. Tokenizer: Convierte palabras a n√∫meros (ej: "excelente" -> 5)
           - Esto es necesario porque las redes neuronales solo procesan n√∫meros
           - NO es un diccionario de sentimientos, solo un mapeo palabra->n√∫mero
        3. Label encoder: Convierte etiquetas a n√∫meros (ej: "positivo" -> 0)
           - Solo para entrenamiento, NO para predicci√≥n
        4. La red neuronal LSTM hace la clasificaci√≥n real en predict()
        """
        # Logging m√≠nimo para ahorrar memoria durante predicci√≥n
        if labels:
            print(f"üîç [DEBUG] prepare_data() entrenamiento: {len(texts)} textos")
        # Si no hay labels, es predicci√≥n - logging m√≠nimo
        
        if not texts:
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        # 1. Limpiar textos (SOLO limpieza, NO clasificaci√≥n)
        cleaned_texts = [self.clean_text(text) if text else "" for text in texts]
        
        # 2. Tokenizar: Convertir palabras a n√∫meros
        # ‚ö†Ô∏è El tokenizer.word_index es un VOCABULARIO (mapeo palabra->n√∫mero)
        # NO es un diccionario de sentimientos. Ejemplo: {"excelente": 5, "malo": 12}
        # Las redes neuronales necesitan n√∫meros, no texto
        if labels:
            # Si hay etiquetas, estamos entrenando, ajustar tokenizer
            self.tokenizer.fit_on_texts(cleaned_texts)
            if len(self.tokenizer.word_index) > 0:
                print(f"üîç [DEBUG] Tokenizer entrenado: vocab_size={len(self.tokenizer.word_index)}")
        elif not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            raise ValueError("El tokenizer no est√° entrenado. Debe entrenar el modelo primero.")
        
        # Convertir textos a secuencias de n√∫meros
        # Ejemplo: "excelente servicio" -> [5, 23] (n√∫meros, no sentimientos)
        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)
        
        # Asegurar que todas las secuencias tengan al menos un elemento (OOV token)
        sequences = [seq if seq else [1] for seq in sequences]
        
        # Hacer padding (rellenar secuencias para que tengan la misma longitud)
        padded_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post', truncating='post')
        
        if labels:
            # 3. Label encoder: Convertir etiquetas a n√∫meros (SOLO para entrenamiento)
            # Ejemplo: "positivo" -> 0, "negativo" -> 1, "neutral" -> 2
            # ‚ö†Ô∏è Esto NO clasifica, solo convierte etiquetas a n√∫meros para entrenar
            encoded_labels = self.label_encoder.fit_transform(labels)
            # Mostrar distribuci√≥n de etiquetas (logging m√≠nimo)
            unique_encoded, counts_encoded = np.unique(encoded_labels, return_counts=True)
            label_names_encoded = self.label_encoder.inverse_transform(unique_encoded)
            print(f"üîç [DEBUG] Datos preparados: shape={padded_sequences.shape}, Distribuci√≥n: {dict(zip(label_names_encoded, counts_encoded))}")
            return padded_sequences, encoded_labels
        
        return padded_sequences
    
    def build_model(self, vocab_size: int, num_classes: int):
        """
        Construir red neuronal LSTM basada en texto.
        
        ‚ö†Ô∏è IMPORTANTE: Esta es una RED NEURONAL REAL (LSTM) que aprende patrones.
        NO hay reglas hardcodeadas, NO hay diccionarios de sentimientos.
        
        Arquitectura de la red neuronal:
        1. Embedding: Convierte n√∫meros de palabras a vectores (representaci√≥n sem√°ntica)
        2. LSTM: Procesa secuencias de palabras y aprende patrones temporales
        3. Dense + Dropout: Capas de aprendizaje que extraen caracter√≠sticas
        4. Dense (softmax): Capa de salida que clasifica en 3 clases (positivo/negativo/neutral)
        
        La red neuronal APRENDE durante el entrenamiento qu√© combinaciones de palabras
        indican sentimientos positivos, negativos o neutrales.
        """
        print(f"üîç [DEBUG] Construyendo modelo: vocab_size={vocab_size}, num_classes={num_classes}")
        print(f"üîç [DEBUG] Par√°metros del modelo: max_words={self.max_words}, max_len={self.max_len}")
        
        # üß† RED NEURONAL LSTM - Aprende patrones, no reglas hardcodeadas
        from tensorflow.keras.initializers import GlorotUniform
        
        # Modelo optimizado para mejor aprendizaje (aumentado de tama√±o m√≠nimo)
        # Balance entre memoria y capacidad de aprendizaje
        effective_vocab_size = min(vocab_size + 1, self.max_words + 1)
        model = Sequential([
            # Capa 1: Embedding - Convierte n√∫meros a vectores sem√°nticos
            Embedding(effective_vocab_size, 16, mask_zero=True),  # 16 dimensiones (aumentado de 6)
            # Capa 2: LSTM - Procesa secuencias y aprende patrones temporales
            LSTM(8, dropout=0.2, recurrent_dropout=0.2),  # 8 unidades (aumentado de 3) con dropout
            # Capa 3: Dense - Extrae caracter√≠sticas aprendidas
            Dense(16, activation='relu'),   # 16 unidades (aumentado de 6)
            # Capa 4: Dropout - Previene sobreajuste
            Dropout(0.3),  # Dropout para regularizaci√≥n
            # Capa 5: Dense (softmax) - Clasifica en 3 clases (positivo/negativo/neutral)
            Dense(num_classes, activation='softmax')  # Salida (3 clases)
        ])
        print(f"üîç [DEBUG] Vocabulario: {effective_vocab_size}, Modelo mejorado: Embedding(16), LSTM(8), Dense(16)")
        
        print(f"üîç [DEBUG] Modelo construido, compilando...")
        # Compilar modelo neuronal con learning rate m√°s conservador para mejor convergencia
        from tensorflow.keras.optimizers import Adam
        optimizer = Adam(learning_rate=0.001)  # Learning rate m√°s conservador (0.001) para mejor convergencia
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy'],
            run_eagerly=False  # Deshabilitar eager mode para mejor rendimiento y convergencia
        )
        
        # NO contar par√°metros aqu√≠ - el modelo a√∫n no est√° "built"
        # Los par√°metros se contar√°n despu√©s del primer fit() cuando el modelo se construya autom√°ticamente
        print(f"üîç [DEBUG] Modelo compilado correctamente (run_eagerly=False)")
        
        return model
    
    def train(self, texts: List[str], labels: List[str], epochs=10, batch_size=32):
        """Entrenar modelo - Versi√≥n ULTRA-LIGERA para Render (512 MB limit)"""
        import tensorflow as tf
        import gc  # Para limpiar memoria
        
        print(f"üìä Preparando datos: {len(texts)} textos, {len(set(labels))} clases")
        X, y = self.prepare_data(texts, labels)
        
        # Mostrar distribuci√≥n de etiquetas ANTES de reducir
        unique_labels, counts = np.unique(y, return_counts=True)
        label_names = self.label_encoder.inverse_transform(unique_labels)
        print(f"üîç [DEBUG] Distribuci√≥n de etiquetas ANTES de reducir:")
        for label_name, count in zip(label_names, counts):
            print(f"   - {label_name}: {count} muestras")
        
        # USAR m√°s datos para mejor aprendizaje (pero sin exceder memoria)
        # Aumentar muestras para mejor precisi√≥n con p√°rrafos largos
        max_samples = 1000  # Usar 1000 muestras para mejor aprendizaje de patrones (dataset estructurado)
        if len(X) > max_samples:
            print(f"‚ö†Ô∏è Reduciendo datos de {len(X)} a {max_samples} para ahorrar memoria...")
            
            # MEZCLAR datos ANTES de reducir para mantener balance de clases
            indices = np.arange(len(X))
            np.random.seed(42)  # Semilla fija para reproducibilidad
            np.random.shuffle(indices)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            # Asegurar balance de clases al reducir
            unique_labels_all = np.unique(y_shuffled)
            num_classes_available = len(unique_labels_all)
            samples_per_class = max_samples // num_classes_available
            min_samples_per_class = max(1, samples_per_class - 2)  # Al menos samples_per_class-2 por clase
            
            print(f"üîç [DEBUG] Intentando balancear: ~{samples_per_class} muestras por clase de {num_classes_available} clases")
            
            # Recopilar muestras balanceadas
            X_balanced = []
            y_balanced = []
            samples_taken_per_class = {int(label): 0 for label in unique_labels_all}
            used_indices = set()
            
            # Primero, tomar muestras balanceadas de cada clase
            for label in unique_labels_all:
                label_int = int(label)
                label_indices = np.where(y_shuffled == label)[0]
                np.random.shuffle(label_indices)
                
                for idx in label_indices[:min_samples_per_class]:
                    if len(X_balanced) >= max_samples:
                        break
                    if idx not in used_indices:
                        X_balanced.append(X_shuffled[idx])
                        y_balanced.append(y_shuffled[idx])
                        used_indices.add(idx)
                        samples_taken_per_class[label_int] += 1
                
                if len(X_balanced) >= max_samples:
                    break
            
            # Si a√∫n hay espacio, tomar muestras adicionales de manera aleatoria
            remaining_indices = [i for i in range(len(X_shuffled)) if i not in used_indices]
            np.random.shuffle(remaining_indices)
            
            for idx in remaining_indices:
                if len(X_balanced) >= max_samples:
                    break
                X_balanced.append(X_shuffled[idx])
                y_balanced.append(y_shuffled[idx])
                used_indices.add(idx)
            
            X = np.array(X_balanced)
            y = np.array(y_balanced)
            
            # Validar balance de clases DESPU√âS de reducir
            unique_labels_reduced, counts_reduced = np.unique(y, return_counts=True)
            label_names_reduced = self.label_encoder.inverse_transform(unique_labels_reduced)
            print(f"üîç [DEBUG] Distribuci√≥n de etiquetas DESPU√âS de reducir (balanceado):")
            for label_name, count in zip(label_names_reduced, counts_reduced):
                print(f"   - {label_name}: {count} muestras")
        else:
            # MEZCLAR datos antes de entrenar para mejor aprendizaje
            print("üîç [DEBUG] Mezclando datos antes de entrenar...")
            indices = np.arange(len(X))
            np.random.seed(42)  # Semilla fija para reproducibilidad
            np.random.shuffle(indices)
            X = X[indices]
            y = y[indices]
            print(f"‚úÖ [DEBUG] Datos mezclados: {len(X)} muestras")
        
        # SIEMPRE entrenar sin validaci√≥n para m√°xima velocidad y menor uso de memoria
        # Con pocos datos, la validaci√≥n no es necesaria y solo ralentiza
        X_train, y_train = X, y
        X_val, y_val = X, y
        use_validation = False
        print(f"üîç [DEBUG] Entrenando SIN validaci√≥n para m√°xima velocidad y menor memoria")
        
        vocab_size = len(self.tokenizer.word_index)
        num_classes = len(self.label_encoder.classes_)
        print(f"üìä Vocabulario: {vocab_size} palabras, Clases: {num_classes}")
        print(f"üìä Datos entrenamiento: {len(X_train)}, Validaci√≥n: {len(X_val) if use_validation else 'N/A'}")
        
        # Validar balance de clases en datos de entrenamiento
        unique_labels_train, counts_train = np.unique(y_train, return_counts=True)
        label_names_train = self.label_encoder.inverse_transform(unique_labels_train)
        print(f"üîç [DEBUG] Distribuci√≥n de etiquetas en datos de ENTRENAMIENTO:")
        for label_name, count in zip(label_names_train, counts_train):
            print(f"   - {label_name}: {count} muestras")
        
        # Verificar que haya al menos una muestra de cada clase en entrenamiento
        if len(unique_labels_train) < num_classes:
            print(f"‚ö†Ô∏è [DEBUG] ADVERTENCIA: Solo hay {len(unique_labels_train)} clases en entrenamiento, esperadas {num_classes}")
            print(f"‚ö†Ô∏è [DEBUG] Esto puede afectar la precisi√≥n del modelo")
        else:
            print(f"‚úÖ [DEBUG] Todas las clases ({num_classes}) est√°n representadas en los datos de entrenamiento")
        
        # Limpiar memoria ANTES de construir modelo (CR√çTICO para Render 512 MB)
        print("üîç [DEBUG] Limpiando memoria antes de construir modelo...")
        import tensorflow as tf
        tf.keras.backend.clear_session()  # Limpiar sesi√≥n de Keras antes
        gc.collect()  # Recolectar basura
        print("‚úÖ [DEBUG] Memoria limpiada antes de construir modelo")
        
        print("üîç [DEBUG] Construyendo modelo...")
        build_start = time.time()
        self.model = self.build_model(vocab_size, num_classes)
        build_time = time.time() - build_start
        print(f"‚úÖ [DEBUG] Modelo construido en {build_time:.2f}s")
        
        # OPTIMIZACI√ìN: Balance entre memoria y aprendizaje
        actual_epochs = 15  # Aumentar √©pocas para mejor aprendizaje (aumentado de 5)
        # Batch size balanceado para mejor aprendizaje
        actual_batch_size = min(8, len(X_train))  # Batch size aumentado para mejor estabilidad (aumentado de 3)
        print(f"üîç [DEBUG] Batch size: {actual_batch_size}, √âpocas: {actual_epochs} (optimizado para mejor aprendizaje)")
        
        print(f"üöÄ Iniciando entrenamiento: {actual_epochs} √©pocas (reducido de {epochs}), batch_size={actual_batch_size} (ajustado de {batch_size})")
        print(f"üìä Datos de entrenamiento: {len(X_train)} muestras")
        print(f"üìä Shape de X_train: {X_train.shape}, Shape de y_train: {y_train.shape}")
        
        # Callbacks simples para entrenamiento
        fit_kwargs = {
            'epochs': actual_epochs,
            'batch_size': actual_batch_size,
            'verbose': 1,  # Mostrar progress (se cambia a 1 en fit())
            'callbacks': []  # Sin callbacks complejos para velocidad
        }
        
        # NO construir modelo expl√≠citamente - ahorra memoria
        # El modelo se construir√° autom√°ticamente en el primer fit()
        print("üîç [DEBUG] El modelo se construir√° autom√°ticamente en el primer fit()")
        
        # Entrenamiento SIMPLIFICADO - sin validaci√≥n, sin callbacks, m√°ximo velocidad
        print("üîç [DEBUG] Llamando a model.fit() sin validaci√≥n...")
        print(f"üîç [DEBUG] X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
        print(f"üîç [DEBUG] Par√°metros: epochs={actual_epochs}, batch_size={actual_batch_size}, samples={len(X_train)}")
        
        # Flush stdout para asegurar que los logs se muestren
        import sys
        sys.stdout.flush()
        
        print(f"üöÄ [DEBUG] INICIANDO model.fit() - entrenamiento con {actual_epochs} √©pocas...")
        sys.stdout.flush()
        
        fit_start = time.time()
        history = None
        try:
            # Entrenamiento con verbose para ver accuracy
            fit_kwargs['verbose'] = 1  # Mostrar progress para ver si est√° aprendiendo
            history = self.model.fit(X_train, y_train, **fit_kwargs)
            fit_time = time.time() - fit_start
            print(f"‚úÖ [DEBUG] model.fit() completado en {fit_time:.2f}s")
            
            # Verificar accuracy final
            if hasattr(history, 'history') and 'accuracy' in history.history:
                final_accuracy = history.history['accuracy'][-1]
                final_loss = history.history['loss'][-1] if 'loss' in history.history else None
                print(f"üìä [DEBUG] Accuracy final del entrenamiento: {final_accuracy:.4f}")
                if final_loss:
                    print(f"üìä [DEBUG] Loss final del entrenamiento: {final_loss:.4f}")
                if final_accuracy < 0.6:
                    print(f"‚ö†Ô∏è [DEBUG] ADVERTENCIA: Accuracy baja ({final_accuracy:.4f}), el modelo podr√≠a no estar aprendiendo bien")
                elif final_accuracy < 0.8:
                    print(f"‚úÖ [DEBUG] Accuracy aceptable: {final_accuracy:.4f} (mejorable pero funcional)")
                else:
                    print(f"‚úÖ [DEBUG] Accuracy excelente: {final_accuracy:.4f}")
            else:
                print(f"‚ö†Ô∏è [DEBUG] No se pudo obtener accuracy del historial")
            
            sys.stdout.flush()
        except Exception as fit_error:
            fit_time = time.time() - fit_start
            print(f"‚ùå [DEBUG] ERROR en model.fit() despu√©s de {fit_time:.2f}s: {str(fit_error)}")
            print(f"üîç [DEBUG] Tipo de error: {type(fit_error).__name__}")
            import traceback
            traceback.print_exc()
            sys.stdout.flush()
            raise
        
        # Ahora s√≠ podemos contar los par√°metros (el modelo ya est√° "built" despu√©s del fit)
        try:
            total_params = self.model.count_params()
            print(f"üìä [DEBUG] Modelo entrenado - Total de par√°metros: {total_params:,}")
        except Exception as e:
            print(f"‚ö†Ô∏è [DEBUG] No se pudo contar par√°metros: {e}")
        
        print(f"‚úÖ Entrenamiento completado (sin validaci√≥n)")
        
        # Validar que el modelo est√© correctamente entrenado
        print("üîç [DEBUG] Validando modelo despu√©s del entrenamiento...")
        if self.model is None:
            raise ValueError("El modelo no existe despu√©s del entrenamiento")
        
        print("üîç [DEBUG] Marcando modelo como entrenado...")
        self.is_trained = True
        print(f"‚úÖ [DEBUG] Modelo marcado como entrenado: is_trained={self.is_trained}")
        print(f"üîç [DEBUG] Modelo existe: {self.model is not None}")
        print(f"üîç [DEBUG] Tokenizer tiene word_index: {hasattr(self.tokenizer, 'word_index') and len(self.tokenizer.word_index) > 0}")
        print(f"üîç [DEBUG] Label encoder tiene classes: {hasattr(self.label_encoder, 'classes_') and len(self.label_encoder.classes_) > 0}")
        
        # NO hacer prueba r√°pida para ahorrar memoria
        # El modelo ya est√° entrenado y validado por el accuracy del entrenamiento
        print("üîç [DEBUG] Prueba r√°pida omitida para ahorrar memoria")
        
        # Limpiar memoria despu√©s de validar (CR√çTICO para Render 512 MB)
        print("üîç [DEBUG] Limpiando memoria despu√©s de entrenar...")
        # NO eliminar history aqu√≠ porque se necesita devolver
        # Solo limpiar otras variables temporales
        gc.collect()  # Recolectar basura de Python
        print("‚úÖ [DEBUG] Memoria limpiada (modelo preservado)")
        
        # Devolver history si existe, sino devolver None
        return history if history is not None else None
    
    def predict(self, texts: List[str]) -> List[Dict]:
        """
        Predecir sentimiento usando SOLO red neuronal LSTM.
        
        ‚ö†Ô∏è IMPORTANTE: Esta funci√≥n usa 100% red neuronal LSTM para clasificar.
        NO hay reglas hardcodeadas, NO hay diccionarios de sentimientos.
        
        Flujo de predicci√≥n:
        1. Limpia el texto (encoding, normalizaci√≥n)
        2. Tokeniza (convierte palabras a n√∫meros)
        3. Pasa por la red neuronal LSTM (aqu√≠ se hace la clasificaci√≥n)
        4. La red neuronal devuelve probabilidades (ej: [0.1, 0.8, 0.1] = negativo)
        5. Se convierte el n√∫mero de clase a etiqueta (ej: 1 -> "negativo")
        
        La clasificaci√≥n real ocurre en la l√≠nea: predictions = self.model.predict(X)
        La red neuronal LSTM aprendi√≥ los patrones durante el entrenamiento.
        """
        # Validaci√≥n r√°pida (sin logs para mejor rendimiento)
        if not self.is_trained or not self.model:
            raise ValueError("El modelo no est√° listo. Por favor, espera unos momentos.")
        
        if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            raise ValueError("El tokenizer no est√° listo. Por favor, espera unos momentos.")
        
        if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
            raise ValueError("El label encoder no est√° listo. Por favor, espera unos momentos.")
        
        if not texts:
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        try:
            # 1. Preparar datos: Convertir texto a n√∫meros (NO clasifica, solo convierte)
            X = self.prepare_data(texts)
            # Limpiar memoria inmediatamente despu√©s de preparar datos
            import gc
            gc.collect()
            
            # Verificar que tenemos datos v√°lidos
            if X.shape[0] == 0:
                raise ValueError("No se pudieron preparar los datos para predicci√≥n")
            
            # 2. üß† AQU√ç ES DONDE LA RED NEURONAL CLASIFICA
            # La red neuronal LSTM procesa los n√∫meros y devuelve probabilidades
            # Ejemplo: [0.1, 0.8, 0.1] = 80% negativo, 10% positivo, 10% neutral
            # NO hay reglas hardcodeadas, TODO es aprendizaje neuronal
            predictions = self.model.predict(X, batch_size=1, verbose=0)
            
            # Validar predicciones
            if predictions is None or len(predictions) == 0:
                raise ValueError("El modelo no devolvi√≥ predicciones")
            
            # 3. Procesar predicciones de la red neuronal
            # np.argmax encuentra la clase con mayor probabilidad (la que eligi√≥ la red neuronal)
            predicted_classes = np.argmax(predictions, axis=1)
            # Convertir n√∫mero de clase a etiqueta (ej: 1 -> "negativo")
            predicted_labels = self.label_encoder.inverse_transform(predicted_classes)
            # Obtener la confianza (probabilidad m√°xima)
            confidence = np.max(predictions, axis=1)
            
            results = []
            for i, text in enumerate(texts):
                if i >= len(predicted_labels):
                    label = 'neutral'
                    score = 0.5
                else:
                    label = predicted_labels[i]
                    score = float(confidence[i])
                
                if label == 'positivo':
                    sentiment = 'positivo'
                    emoji = 'üü¢'
                    score_value = score
                elif label == 'negativo':
                    sentiment = 'negativo'
                    emoji = 'üî¥'
                    score_value = -score
                else:
                    sentiment = 'neutral'
                    emoji = 'üü°'
                    score_value = 0.0
                
                results.append({
                    'text': text,
                    'sentiment': sentiment,
                    'score': round(score_value, 3),
                    'emoji': emoji,
                    'confidence': round(score, 3)
                })
            
            print(f"‚úÖ [DEBUG] Predicci√≥n completada: {len(results)} resultado(s)")
            return results
            
        except ValueError as e:
            # Re-lanzar ValueError con mensaje claro
            error_msg = str(e)
            print(f"‚ùå [DEBUG] ValueError en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"Error en predicci√≥n de red neuronal: {str(e)}"
            print(f"‚ùå [DEBUG] Exception en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
    
    def predict_single(self, text: str) -> Dict:
        """Predecir sentimiento de un solo texto"""
        try:
            results = self.predict([text])
            if not results or len(results) == 0:
                raise ValueError("No se obtuvieron resultados de la predicci√≥n")
            return results[0]
        except Exception as e:
            raise
    
    def load_model(self, model_path: str = 'app/ml_models/sentiment_model.keras'):
        """Cargar modelo pre-entrenado - Descarga autom√°tica desde GitHub Releases si no existe"""
        # Asegurar que el directorio existe
        model_dir = os.path.dirname(model_path)
        if not os.path.exists(model_dir):
            os.makedirs(model_dir, exist_ok=True)
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        # URLs para descargar modelo pre-entrenado desde GitHub Releases
        # ACTUALIZA ESTAS URLs con las URLs reales despu√©s de subir a GitHub Releases
        MODEL_URL = os.getenv(
            'MODEL_URL', 
            'https://github.com/crisncr/inteligenciaArtificial/releases/download/v1.0.0/sentiment_model.keras'
        )
        TOKENIZER_URL = os.getenv(
            'TOKENIZER_URL',
            'https://github.com/crisncr/inteligenciaArtificial/releases/download/v1.0.0/tokenizer.pkl'
        )
        LABEL_ENCODER_URL = os.getenv(
            'LABEL_ENCODER_URL',
            'https://github.com/crisncr/inteligenciaArtificial/releases/download/v1.0.0/label_encoder.pkl'
        )
        
        def download_file(url: str, filepath: str) -> bool:
            """Descargar archivo desde URL - Optimizado para memoria"""
            try:
                import requests
                print(f"üì• Descargando {os.path.basename(filepath)} desde GitHub Releases...")
                # Timeout m√°s corto y stream para ahorrar memoria
                response = requests.get(url, timeout=30, stream=True)
                response.raise_for_status()
                
                os.makedirs(os.path.dirname(filepath), exist_ok=True)
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                # Descargar en chunks peque√±os para ahorrar memoria
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                        # Limpiar memoria peri√≥dicamente durante la descarga
                        if downloaded % (1024 * 1024) == 0:  # Cada 1MB
                            import gc
                            gc.collect()
                
                # Limpiar memoria despu√©s de descargar
                import gc
                del response
                gc.collect()
                
                file_size_kb = downloaded / 1024
                print(f"‚úÖ {os.path.basename(filepath)} descargado correctamente ({file_size_kb:.1f} KB)")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è No se pudo descargar {os.path.basename(filepath)}: {str(e)}")
                print(f"üîç URL intentada: {url}")
                try:
                    if os.path.exists(filepath):
                        os.remove(filepath)
                except:
                    pass
                # Limpiar memoria en caso de error
                import gc
                gc.collect()
                return False
        
        # Verificar qu√© archivos faltan
        missing_files = []
        if not os.path.exists(model_path):
            missing_files.append(('Modelo', MODEL_URL, model_path))
        if not os.path.exists(tokenizer_path):
            missing_files.append(('Tokenizer', TOKENIZER_URL, tokenizer_path))
        if not os.path.exists(label_encoder_path):
            missing_files.append(('Label Encoder', LABEL_ENCODER_URL, label_encoder_path))
        
        # Descargar archivos faltantes
        if missing_files:
            print(f"üì• Descargando {len(missing_files)} archivo(s) del modelo pre-entrenado desde GitHub Releases...")
            downloaded_count = 0
            for name, url, filepath in missing_files:
                print(f"üì• Descargando {name}...")
                if download_file(url, filepath):
                    downloaded_count += 1
                else:
                    print(f"‚ùå Error al descargar {name}")
            
            if downloaded_count < len(missing_files):
                print(f"‚ùå ERROR: Solo se descargaron {downloaded_count}/{len(missing_files)} archivos.")
                print("‚ùå NO se puede entrenar el modelo en producci√≥n (consume demasiada memoria)")
                print("üí° SOLUCI√ìN: Sube los archivos del modelo a GitHub Releases")
                print("üìã Ver train_model_local.py para instrucciones")
                # Limpiar archivos parcialmente descargados
                for name, url, filepath in missing_files:
                    if os.path.exists(filepath):
                        try:
                            os.remove(filepath)
                        except:
                            pass
                # NO ENTRENAR - Lanzar error en lugar de entrenar
                raise ValueError(
                    f"No se pudieron descargar los archivos del modelo desde GitHub Releases. "
                    f"Archivos faltantes: {len(missing_files) - downloaded_count}. "
                    f"Por favor, aseg√∫rate de que los archivos est√©n disponibles en GitHub Releases o "
                    f"entrena el modelo localmente y s√∫belo a GitHub Releases."
                )
            else:
                print("‚úÖ Todos los archivos del modelo se descargaron correctamente desde GitHub Releases")
                print("‚úÖ El modelo NO se entrenar√°, se usar√° el modelo pre-entrenado")
        
        # Intentar cargar modelo existente (local o descargado)
        if os.path.exists(model_path) and os.path.exists(tokenizer_path) and os.path.exists(label_encoder_path):
            try:
                # Cargar modelo en formato .keras (compatible con Keras 3.x)
                try:
                    # Intentar cargar directamente (formato .keras es m√°s compatible)
                    self.model = load_model(model_path)
                except Exception as load_error:
                    # Si falla, intentar cargar sin compilaci√≥n
                    self.model = load_model(model_path, compile=False)
                    # Recompilar el modelo
                    from tensorflow.keras.optimizers import Adam
                    self.model.compile(
                        optimizer=Adam(learning_rate=0.001),
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy']
                    )
                
                # Cargar tokenizer y label encoder (optimizado para memoria)
                with open(tokenizer_path, 'rb') as f:
                    self.tokenizer = pickle.load(f)
                
                with open(label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                
                # Limpiar memoria despu√©s de cargar
                import gc
                gc.collect()
                
                # Verificar que el modelo est√° correctamente cargado
                if self.model is None:
                    raise ValueError("El modelo no se carg√≥ correctamente")
                if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
                    raise ValueError("El tokenizer no se carg√≥ correctamente")
                if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                    raise ValueError("El label encoder no se carg√≥ correctamente")
                
                # Marcar modelo como entrenado (sin validaci√≥n con predicci√≥n para mejor rendimiento)
                self.is_trained = True
                
                return
            except Exception as e:
                print(f"‚ö†Ô∏è Error al cargar modelo pre-entrenado: {e}")
                import traceback
                traceback.print_exc()
                print("üîÑ Se crear√° un nuevo modelo...")
                # Limpiar archivos corruptos si existen
                try:
                    if os.path.exists(model_path):
                        os.remove(model_path)
                    if os.path.exists(tokenizer_path):
                        os.remove(tokenizer_path)
                    if os.path.exists(label_encoder_path):
                        os.remove(label_encoder_path)
                except:
                    pass
        
        # Si no existe o fall√≥ cargar, NO ENTRENAR - Lanzar error
        # El entrenamiento consume demasiada memoria (>512MB) en Render
        print("=" * 60)
        print("‚ùå ERROR: No se pudo cargar el modelo pre-entrenado")
        print("=" * 60)
        print("‚ùå NO se puede entrenar el modelo en producci√≥n (l√≠mite de 512MB de memoria)")
        print("üí° SOLUCI√ìN:")
        print("   1. Entrena el modelo localmente: python train_model_local.py")
        print("   2. Sube los archivos a GitHub Releases")
        print("   3. Aseg√∫rate de que las URLs en load_model() sean correctas")
        print("üìã Ver train_model_local.py y GUIA_GITHUB_RELEASES.md para instrucciones")
        raise ValueError(
            "No se pudo cargar el modelo pre-entrenado y no se puede entrenar en producci√≥n "
            "(l√≠mite de memoria: 512MB). Por favor, aseg√∫rate de que los archivos del modelo "
            "est√©n disponibles en GitHub Releases. Entrena el modelo localmente y s√∫belo a "
            "GitHub Releases antes de desplegar."
        )
    
    def _label_text_with_keywords(self, text: str) -> str:
        """
        Etiqueta un texto como positivo, negativo o neutral usando palabras clave.
        Versi√≥n mejorada para detectar mejor los comentarios negativos.
        Usado para etiquetar textos de Hugging Face que no tienen etiquetas de sentimiento.
        """
        if not text:
            return 'neutral'
        
        text_lower = text.lower()
        
        # Palabras clave positivas (EXPANDIDO)
        positive_keywords = [
            'excelente', 'bueno', 'buena', 'genial', 'perfecto', 'perfecta',
            'incre√≠ble', 'maravilloso', 'fant√°stico', 'delicioso', 'deliciosa',
            'agradable', 'acogedor', 'acogedora', 'limpio', 'limpia', 'bonito', 'bonita',
            'recomiendo', 'recomendado', 'recomendada', 'satisfecho', 'satisfecha',
            'me encanta', 'me encant√≥', 'me encanto', 'super√≥', 'supero', 'super', 's√∫per',
            'feliz', 'contento', 'contenta', 'alegre', 'amable', 'atento', 'atenta',
            'r√°pido', 'r√°pida', 'eficiente', 'profesional', 'calidad', 'precio', 'barato', 'barata',
            'vale la pena', 'vali√≥ la pena', 'valio la pena', 'volver√©', 'volvere',
            'satisfactorio', 'satisfactoria', 'recomendable',
            # Palabras adicionales para casos espec√≠ficos
            'f√°cil', 'facil', 'f√°cil de usar', 'facil de usar',
            'atenci√≥n r√°pida', 'atencion rapida', 'atenci√≥n eficiente', 'atencion eficiente',
            'r√°pida y eficiente', 'rapida y eficiente', 'r√°pido y eficiente', 'rapido y eficiente'
        ]
        
        # Palabras clave negativas (EXPANDIDO para detectar mejor los negativos)
        negative_keywords = [
            # Calificaciones negativas directas
            'malo', 'mala', 'p√©simo', 'p√©sima', 'terrible', 'horrible', 'decepcionante',
            'decepcionado', 'decepcionada', 'decepci√≥n', 'decepcion',
            # Problemas de calidad/estado
            'roto', 'rota', 'da√±ado', 'da√±ada', 'defectuoso', 'defectuosa', 
            'incompleto', 'incompleta', 'en mal estado', 'mal estado', 
            'defectos', 'defecto', 'da√±os', 'da√±o',
            # Problemas de temperatura/sabor
            'fr√≠o', 'fr√≠a', 'sin sabor', 'horrible sabor', 'sabor horrible',
            'comida fr√≠a', 'comida fria',
            # Problemas de tiempo/demora
            'tarde', 'demor√≥', 'demoro', 'demorado', 'demorada', 'retraso', 
            'retrasado', 'retrasada', 'se demor√≥', 'se demoro', 
            'demor√≥ demasiado', 'demoro demasiado', 'tard√≥', 'tardo',
            'lento', 'lenta', 'demasiado lento', 'demasiado lenta',
            'lleg√≥ tarde', 'llego tarde', 'con retraso',
            # Problemas de entrega/env√≠o
            'desastre', 'perdido', 'perdida', 'se perdi√≥', 'se perdio', 
            'no lleg√≥', 'no llego', 'lleg√≥ incompleto', 'llego incompleto', 
            'lleg√≥ en mal estado', 'llego en mal estado', 'lleg√≥ roto', 'llego roto',
            'el env√≠o se perdi√≥', 'el envio se perdio', 'el env√≠o se demor√≥',
            'el envio se demoro', 'la entrega fue un desastre',
            # Problemas de servicio/atenci√≥n
            'grosero', 'grosera', 'mala atenci√≥n', 'p√©sima atenci√≥n', 
            'p√©simo servicio', 'poco atento', 'poca atenci√≥n', 
            'no respondi√≥', 'no respondio', 'nunca respondi√≥', 'nunca respondio',
            'no funciona', 'no funciona bien', 'no cumple',
            'mala comunicaci√≥n', 'p√©sima comunicaci√≥n',
            # Negaciones y rechazo
            'no recomiendo', 'no recomendaria', 'nunca volver√©', 'nunca volvere',
            'no volver√©', 'no volvere', 'no comprar√≠a', 'no compraria',
            'no lo recomiendo', 'no lo recomendaria', 'no lo recomendar√≠a',
            'no recib√≠', 'no recibi', 'no recibi√≥', 'no recibio',
            # Problemas y quejas
            'problema', 'problemas', 'queja', 'quejas', 'reclamo', 'reclamos',
            'insatisfecho', 'insatisfecha', 'devoluci√≥n', 'devolver', 'reembolso',
            # Otros negativos
            'lleno de errores', 'errores', 'no cumple expectativas', 
            'no cumpli√≥', 'no cumplio', 'defraudado', 'defraudada',
            # Frases negativas comunes
            'muy mala', 'muy malo', 'muy mal', 'p√©simo servicio', 
            'terrible experiencia', 'no funcion√≥', 'no funciono',
            'no sirve', 'no sirvi√≥', 'no sirvio', 'horrible experiencia',
            'una p√©sima experiencia', 'una pesima experiencia', 'p√©sima experiencia',
            # Negaciones espec√≠ficas con palabras positivas
            'no vale', 'no vale la pena', 'no vale la calidad', 'no vale el precio',
            'no es bueno', 'no es buena', 'no es excelente', 'no es genial'
        ]
        
        # Contar palabras positivas y negativas primero
        positive_count = sum(1 for keyword in positive_keywords if keyword in text_lower)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text_lower)
        
        # Detectar negaciones que cambian el sentido (ej: "no es bueno" = negativo)
        negation_words = ['no', 'nunca', 'jam√°s', 'jamas', 'tampoco', 'ni']
        words = text_lower.split()
        has_negation_near_positive = False
        has_negation_with_value = False  # Para "no vale"
        
        # Buscar patrones espec√≠ficos de negaci√≥n
        text_lower_clean = ' ' + text_lower + ' '  # Agregar espacios para b√∫squeda exacta
        
        # Detectar "no vale" (ej: "no vale la calidad", "no vale la pena")
        if ' no vale ' in text_lower_clean or text_lower.startswith('no vale ') or text_lower.endswith(' no vale'):
            has_negation_with_value = True
            negative_count += 3  # Peso alto para este patr√≥n
        
        # Buscar patrones como "no es bueno", "nunca fue excelente", etc.
        for i, word in enumerate(words):
            if word in negation_words:
                # Verificar si hay palabra positiva cerca (dentro de 4 palabras)
                context_start = max(0, i-4)
                context_end = min(len(words), i+5)
                context = ' '.join(words[context_start:context_end])
                
                # Palabras positivas que pueden ser negadas
                positive_words_to_check = ['bueno', 'buena', 'excelente', 'genial', 'perfecto', 
                                         'recomiendo', 'satisfecho', 'contento', 'vale', 'vali√≥',
                                         'valio', 'recomendable', '√∫til', 'util']
                
                for pos_word in positive_words_to_check:
                    if pos_word in context:
                        has_negation_near_positive = True
                        break
                
                if has_negation_near_positive:
                    break
        
        # Detectar frases con "muy" + adjetivo positivo/negativo
        if 'muy ' in text_lower:
            muy_index = text_lower.find('muy ')
            if muy_index != -1:
                # Buscar adjetivo despu√©s de "muy" (hasta 5 palabras para capturar contexto)
                rest_of_text = text_lower[muy_index + 4:].split()[0:5]
                rest_text = ' '.join(rest_of_text)
                
                # Adjetivos positivos con "muy"
                muy_positivos = ['amable', 'satisfecho', 'satisfecha', 'contento', 'contenta', 
                               'bueno', 'buena', 'bien', 'f√°cil', 'facil', 'feliz', 'excelente',
                               'buen', 'satisfactorio', 'satisfactoria']
                if any(adj in rest_text for adj in muy_positivos):
                    positive_count += 3  # Peso alto para "muy + positivo"
                
                # Adjetivos negativos con "muy"
                muy_negativos = ['malo', 'mala', 'mal', 'p√©simo', 'pesimo', 'p√©sima', 'pesima',
                               'decepcionado', 'decepcionada', 'insatisfecho', 'insatisfecha']
                if any(adj in rest_text for adj in muy_negativos):
                    negative_count += 3  # Peso alto para "muy + negativo"
        
        # Detectar patrones espec√≠ficos positivos en contexto
        # "atenci√≥n al cliente" + adjetivo positivo
        if 'atenci√≥n' in text_lower or 'atencion' in text_lower:
            if any(pos in text_lower for pos in ['amable', 'r√°pida', 'rapida', 'eficiente', 'buena', 'excelente']):
                positive_count += 2
        
        # "dise√±o" + verbo positivo (ej: "me encant√≥ el dise√±o")
        if 'dise√±o' in text_lower or 'diseno' in text_lower:
            if any(pos in text_lower for pos in ['encant√≥', 'encanto', 'encanta', 'excelente', 'bueno', 'bonito']):
                positive_count += 2
        
        # "proceso" + adjetivo positivo (ej: "f√°cil proceso")
        if 'proceso' in text_lower:
            if any(pos in text_lower for pos in ['f√°cil', 'facil', 'r√°pido', 'rapido', 'sencillo', 'bueno']):
                positive_count += 2
        
        # "compra" + adjetivo positivo (ej: "f√°cil compra", "buena compra")
        if 'compra' in text_lower:
            if any(pos in text_lower for pos in ['f√°cil', 'facil', 'buena', 'buen', 'satisfecho', 'contento']):
                positive_count += 2
        
        # "resultado" + adjetivo positivo (ej: "satisfecho con el resultado")
        if 'resultado' in text_lower:
            if any(pos in text_lower for pos in ['satisfecho', 'satisfecha', 'contento', 'contenta', 'bueno', 'excelente']):
                positive_count += 2
        
        # "app" o "aplicaci√≥n" + adjetivo positivo (ej: "app f√°cil de usar")
        if 'app' in text_lower or 'aplicaci√≥n' in text_lower or 'aplicacion' in text_lower:
            if any(pos in text_lower for pos in ['f√°cil', 'facil', 'r√°pida', 'rapida', 'eficiente', 'buena']):
                positive_count += 2
        
        # Si hay negaci√≥n con "vale", es definitivamente negativo
        if has_negation_with_value:
            return 'negativo'
        
        # Si hay negaci√≥n cerca de palabra positiva, es negativo (ej: "no es bueno")
        if has_negation_near_positive:
            negative_count += 3  # Peso alto para negaciones
        
        # Detectar "p√©sima experiencia" o variantes
        if 'p√©sima experiencia' in text_lower or 'pesima experiencia' in text_lower or \
           'p√©sima' in text_lower and 'experiencia' in text_lower:
            negative_count += 2
        
        # Determinar sentimiento con l√≥gica mejorada
        # Si hay indicadores negativos claros, priorizar negativo
        if negative_count > 0:
            # Si hay m√°s negativos que positivos, o si hay al menos 2 negativos, es negativo
            if negative_count > positive_count or negative_count >= 2:
                return 'negativo'
            # Si hay negativos pero tambi√©n muchos positivos, puede ser positivo
            elif positive_count > negative_count * 2:
                return 'positivo'
        
        # Si hay positivos y no hay negativos, es positivo
        if positive_count > 0 and negative_count == 0:
            return 'positivo'
        
        # Si hay m√°s positivos que negativos, es positivo
        if positive_count > negative_count:
            return 'positivo'
        
        # Si hay negativos y no hay positivos, es negativo
        if negative_count > 0 and positive_count == 0:
            return 'negativo'
        
        # Por defecto, neutral
        return 'neutral'
    
    def _load_huggingface_datasets(self, limite: int = 5000, min_negativos: int = 300) -> List[Dict[str, str]]:
        """
        Carga datasets de Hugging Face en espa√±ol y los etiqueta autom√°ticamente.
        Carga muchos datos hasta encontrar suficientes ejemplos negativos.
        
        Args:
            limite: N√∫mero m√°ximo de comentarios a cargar
            min_negativos: N√∫mero m√≠nimo de comentarios negativos requeridos
            
        Returns:
            Lista de diccionarios con 'valor' (positivo/negativo/neutral) y 'comentario'
        """
        datos = []
        
        try:
            from datasets import load_dataset
            print("üîÑ Cargando dataset de an√°lisis de sentimientos en espa√±ol desde Hugging Face...")
            print(f"üì• Solicitando hasta {limite} comentarios para encontrar al menos {min_negativos} negativos...")
            
            # Intentar cargar diferentes datasets compatibles
            dataset = None
            dataset_name = None
            
            # Opci√≥n 1: Dataset de an√°lisis de sentimientos en textos tur√≠sticos de M√©xico
            try:
                print("üîÑ Intentando cargar: alexcom/analisis-sentimientos-textos-turisitcos-mx-paisV2...")
                dataset = load_dataset("alexcom/analisis-sentimientos-textos-turisitcos-mx-paisV2", split=f"train[:{limite}]")
                dataset_name = "Textos Tur√≠sticos M√©xico"
                print(f"‚úÖ Dataset cargado: {len(dataset)} comentarios disponibles")
            except Exception as e1:
                print(f"‚ö†Ô∏è No se pudo cargar dataset tur√≠stico: {e1}")
                return []
            
            # Procesar cada comentario
            negativos_encontrados = 0
            for item in dataset:
                # Obtener texto del comentario
                texto = item.get('text', item.get('texto', item.get('comentario', 
                        item.get('review_body', item.get('content', item.get('review', ''))))))
                
                # Validar que el texto tenga sentido
                if not texto or not isinstance(texto, str) or len(texto.strip()) < 10:
                    continue
                
                # Limpiar texto
                texto = texto.strip()
                
                # Filtrar comentarios sin sentido
                if not self._is_valid_comment(texto):
                    continue
                
                # Etiquetar usando palabras clave mejoradas
                sentimiento = self._label_text_with_keywords(texto)
                
                # Si es negativo, incrementar contador
                if sentimiento == 'negativo':
                    negativos_encontrados += 1
                
                datos.append({
                    'valor': sentimiento,
                    'comentario': texto
                })
                
                # Si ya tenemos suficientes negativos y suficientes datos totales, podemos parar antes
                # (pero procesamos todos para tener mejor distribuci√≥n)
            
            print(f"‚úÖ {len(datos)} comentarios v√°lidos procesados de {dataset_name}")
            
            # Mostrar distribuci√≥n de sentimientos
            positivo_count = sum(1 for d in datos if d['valor'] == 'positivo')
            negativo_count = sum(1 for d in datos if d['valor'] == 'negativo')
            neutral_count = sum(1 for d in datos if d['valor'] == 'neutral')
            print(f"üìä Distribuci√≥n: {positivo_count} positivos, {negativo_count} negativos, {neutral_count} neutrales")
            
            # Advertir si no hay suficientes negativos
            if negativo_count < min_negativos:
                print(f"‚ö†Ô∏è ADVERTENCIA: Solo se encontraron {negativo_count} comentarios negativos (objetivo: {min_negativos})")
                print(f"üí° El dataset ser√° balanceado con los negativos disponibles")
            else:
                print(f"‚úÖ Se encontraron {negativo_count} comentarios negativos (objetivo: {min_negativos})")
            
        except ImportError:
            print("‚ùå Error: La librer√≠a 'datasets' no est√° instalada.")
            print("üí° Instala con: pip install datasets")
            return []
        except Exception as e:
            print(f"‚ö†Ô∏è Error al cargar dataset desde Hugging Face: {e}")
            print(f"üìã Tipo de error: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return []
        
        return datos
    
    def _create_training_dataset(self) -> List[Dict[str, str]]:
        """
        Crea un dataset estructurado con ~1000 muestras balanceadas.
        Usa solo datos de Hugging Face, balanceando autom√°ticamente las clases.
        Los textos se cargan desde Hugging Face y se etiquetan autom√°ticamente usando palabras clave.
        """
        dataset = []
        
        print("=" * 80)
        print("CARGANDO DATASET DE HUGGING FACE (ESPA√ëOL)")
        print("=" * 80)
        print()
        
        # Cargar muchos datos de Hugging Face para encontrar suficientes negativos
        hf_data = self._load_huggingface_datasets(limite=5000, min_negativos=300)
        
        if not hf_data:
            raise ValueError(
                "No se pudieron cargar datos de Hugging Face. "
                "Aseg√∫rate de tener instalada la librer√≠a 'datasets' (pip install datasets) "
                "y conexi√≥n a internet para descargar el dataset."
            )
        
        # Separar por sentimiento
        hf_positivos = [d for d in hf_data if d['valor'] == 'positivo']
        hf_negativos = [d for d in hf_data if d['valor'] == 'negativo']
        hf_neutrales = [d for d in hf_data if d['valor'] == 'neutral']
        
        print(f"\nüìä Datos disponibles despu√©s de etiquetado: {len(hf_positivos)} positivos, {len(hf_negativos)} negativos, {len(hf_neutrales)} neutrales")
        
        # Balancear: usar la cantidad de negativos como referencia
        # Si hay pocos negativos, usar todos y balancear positivos/neutrales
        import random
        random.seed(42)
        
        if len(hf_negativos) > 0:
            # Usar todos los negativos disponibles (son los m√°s importantes)
            target_negativos = min(len(hf_negativos), 350)
            target_por_clase = target_negativos  # Balancear con la misma cantidad
            
            # Negativos: usar todos los disponibles (hasta el l√≠mite)
            if len(hf_negativos) > target_negativos:
                hf_negativos_selected = random.sample(hf_negativos, target_negativos)
            else:
                hf_negativos_selected = hf_negativos
            
            # Positivos: limitar a la misma cantidad que negativos
            if len(hf_positivos) > target_por_clase:
                hf_positivos_selected = random.sample(hf_positivos, target_por_clase)
            else:
                hf_positivos_selected = hf_positivos
            
            # Neutrales: limitar a la misma cantidad
            if len(hf_neutrales) > target_por_clase:
                hf_neutrales_selected = random.sample(hf_neutrales, target_por_clase)
            else:
                hf_neutrales_selected = hf_neutrales
            
            # Combinar
            dataset.extend(hf_negativos_selected)
            dataset.extend(hf_positivos_selected)
            dataset.extend(hf_neutrales_selected)
            
            print(f"‚úÖ Dataset balanceado seleccionado: {len(hf_negativos_selected)} negativos, {len(hf_positivos_selected)} positivos, {len(hf_neutrales_selected)} neutrales")
        else:
            # Si no hay negativos, usar todos los datos disponibles
            print("‚ö†Ô∏è No se encontraron comentarios negativos, usando todos los datos disponibles")
            dataset.extend(hf_data)
        
        # Eliminar duplicados (mismo comentario)
        seen_comments = set()
        unique_dataset = []
        for item in dataset:
            # Normalizar comentario para comparaci√≥n
            normalized = self._normalize_for_comparison(item['comentario'])
            if normalized not in seen_comments:
                seen_comments.add(normalized)
                unique_dataset.append(item)
        
        dataset = unique_dataset
        
        # Mezclar dataset
        random.seed(42)
        random.shuffle(dataset)
        
        # Limitar a ~1000 muestras si es necesario (mantener balance)
        if len(dataset) > 1000:
            # Mantener proporci√≥n balanceada al limitar
            positive_count = sum(1 for d in dataset if d['valor'] == 'positivo')
            negative_count = sum(1 for d in dataset if d['valor'] == 'negativo')
            neutral_count = sum(1 for d in dataset if d['valor'] == 'neutral')
            
            # Calcular proporciones
            total = len(dataset)
            p_ratio = positive_count / total
            n_ratio = negative_count / total
            neu_ratio = neutral_count / total
            
            # Seleccionar manteniendo proporciones
            target_positive = int(1000 * p_ratio)
            target_negative = int(1000 * n_ratio)
            target_neutral = 1000 - target_positive - target_negative
            
            balanced_dataset = []
            balanced_dataset.extend([d for d in dataset if d['valor'] == 'positivo'][:target_positive])
            balanced_dataset.extend([d for d in dataset if d['valor'] == 'negativo'][:target_negative])
            balanced_dataset.extend([d for d in dataset if d['valor'] == 'neutral'][:target_neutral])
            
            random.shuffle(balanced_dataset)
            dataset = balanced_dataset
        
        # Estad√≠sticas finales
        positive_count = sum(1 for d in dataset if d['valor'] == 'positivo')
        negative_count = sum(1 for d in dataset if d['valor'] == 'negativo')
        neutral_count = sum(1 for d in dataset if d['valor'] == 'neutral')
        with_numbers = sum(1 for d in dataset if any(c.isdigit() for c in d['comentario']))
        
        print()
        print("=" * 80)
        print("RESUMEN DEL DATASET BALANCEADO")
        print("=" * 80)
        print(f"üìä Total de comentarios: {len(dataset)}")
        print(f"üìä   - Positivos: {positive_count} ({positive_count/len(dataset)*100:.1f}%)")
        print(f"üìä   - Negativos: {negative_count} ({negative_count/len(dataset)*100:.1f}%)")
        print(f"üìä   - Neutrales: {neutral_count} ({neutral_count/len(dataset)*100:.1f}%)")
        print(f"üìä   - Comentarios con n√∫meros: {with_numbers} ({with_numbers/len(dataset)*100:.1f}%)")
        print("=" * 80)
        print()
        
        return dataset
    
    def _create_pretrained_model(self):
        """
        Entrenar red neuronal LSTM con dataset real de Hugging Face (~1000 muestras balanceadas).
        Los textos se cargan desde Hugging Face y se etiquetan autom√°ticamente usando palabras clave.
        El dataset se balancea autom√°ticamente para tener distribuci√≥n similar de positivos, negativos y neutrales.
        El modelo aprender√° patrones generales de comentarios reales, mejorando la generalizaci√≥n.
        """
        print("üîç [DEBUG] _create_pretrained_model() iniciado")
        print("üìä Modelo configurado para p√°rrafos largos: max_len=100, max_words=5000")
        print("üîÑ Cargando dataset estructurado balanceado con ~1000 muestras desde Hugging Face...")
        
        # Generar dataset estructurado (valor, comentario)
        dataset = self._create_training_dataset()
        
        if not dataset:
            raise ValueError("No se gener√≥ ning√∫n comentario v√°lido en el dataset")
        
        # Extraer textos y etiquetas del dataset estructurado
        texts = [item["comentario"] for item in dataset]
        labels = [item["valor"] for item in dataset]
        
        print(f"üîÑ Entrenando red neuronal LSTM con {len(texts)} comentarios...")
        print(f"üìä Distribuci√≥n: {labels.count('positivo')} positivos, {labels.count('negativo')} negativos, {labels.count('neutro')} neutrales")
        
        # Entrenamiento con m√°s √©pocas para mejor aprendizaje
        print("üîç [DEBUG] Iniciando entrenamiento...")
        try:
            # Entrenar modelo usando el m√©todo train() existente
            # El m√©todo train() ya maneja la preparaci√≥n de datos, tokenizaci√≥n, etc.
            history = self.train(texts, labels, epochs=15, batch_size=32)
            print("‚úÖ [DEBUG] M√©todo train() completado")
            
            # Validar que el modelo est√° entrenado
            print(f"üîç [DEBUG] Verificando estado del modelo despu√©s del entrenamiento...")
            print(f"üîç [DEBUG] is_trained: {self.is_trained}")
            print(f"üîç [DEBUG] model existe: {self.model is not None}")
            print(f"üîç [DEBUG] tokenizer tiene word_index: {hasattr(self.tokenizer, 'word_index') and self.tokenizer.word_index is not None}")
            print(f"üîç [DEBUG] label_encoder tiene classes: {hasattr(self.label_encoder, 'classes_') and len(self.label_encoder.classes_) > 0}")
            
            if not self.is_trained:
                raise ValueError("El modelo no se marc√≥ como entrenado despu√©s del entrenamiento")
            if not self.model:
                raise ValueError("El modelo no existe despu√©s del entrenamiento")
            
            print("üîç [DEBUG] Guardando modelo...")
            self.save_model()
            print("‚úÖ [DEBUG] Modelo guardado correctamente")
            
            # Validaci√≥n final: hacer una predicci√≥n de prueba
            print("üîç [DEBUG] Haciendo predicci√≥n de prueba despu√©s del entrenamiento...")
            test_result = self.predict_single("excelente servicio")
            print(f"‚úÖ [DEBUG] Predicci√≥n de prueba exitosa: sentiment={test_result.get('sentiment')}, score={test_result.get('score')}")
            
        except Exception as e:
            print(f"‚ùå [DEBUG] Error en _create_pretrained_model: {str(e)}")
            import traceback
            traceback.print_exc()
            self.is_trained = False
            raise
        
        print("‚úÖ Red neuronal LSTM entrenada correctamente")
    
    def save_model(self, model_path: str = 'app/ml_models/sentiment_model.keras'):
        """Guardar modelo en formato .keras (compatible con Keras 3.x)"""
        model_dir = os.path.dirname(model_path)
        os.makedirs(model_dir, exist_ok=True)
        
        if self.model:
            # Guardar en formato .keras (m√°s compatible con Keras 3.x)
            # Keras 3.x infiere autom√°ticamente el formato desde la extensi√≥n .keras
            self.model.save(model_path)
            print(f"‚úÖ Modelo guardado en formato .keras: {model_path}")
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(self.tokenizer, f)
        with open(label_encoder_path, 'wb') as f:
            pickle.dump(self.label_encoder, f)
        
        print(f"‚úÖ Tokenizer guardado en: {tokenizer_path}")
        print(f"‚úÖ Label encoder guardado en: {label_encoder_path}")
