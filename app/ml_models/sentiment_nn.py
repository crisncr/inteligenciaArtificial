import re
import numpy as np
from typing import Dict, List, Tuple
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

class SentimentNeuralNetwork:
    def __init__(self, max_words=800, max_len=35):
        # Red neuronal LSTM basada en texto - Soporta comentarios de hasta 25 palabras
        # max_words: 800 (vocabulario suficiente para comentarios)
        # max_len: 35 (soporta c√≥modamente hasta 25 palabras)
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
        self.label_encoder = LabelEncoder()
        self.model = None
        self.is_trained = False
        
    def clean_text(self, text: str) -> str:
        """Limpieza de texto - Parte 1"""
        if not text:
            return ""
        
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Eliminar URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Eliminar menciones y hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        
        # Eliminar caracteres especiales excepto letras, n√∫meros y espacios
        text = re.sub(r'[^a-z√°√©√≠√≥√∫√±√º\s]', ' ', text)
        
        # Eliminar espacios m√∫ltiples
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def prepare_data(self, texts: List[str], labels: List[str] = None) -> Tuple:
        """Preparar datos para entrenamiento o predicci√≥n"""
        print(f"üîç [DEBUG] prepare_data() llamado con {len(texts)} texto(s), labels={labels is not None}")
        
        if not texts:
            print("‚ùå [DEBUG] Error: lista de textos vac√≠a en prepare_data")
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        # Limpiar textos
        print("üîç [DEBUG] Limpiando textos...")
        cleaned_texts = [self.clean_text(text) if text else "" for text in texts]
        print(f"üîç [DEBUG] Textos limpiados: {[t[:30] + '...' if len(t) > 30 else t for t in cleaned_texts[:3]]}")
        
        # Tokenizar
        if labels:
            # Si hay etiquetas, estamos entrenando, ajustar tokenizer
            print("üîç [DEBUG] Entrenando tokenizer...")
            self.tokenizer.fit_on_texts(cleaned_texts)
            print(f"üîç [DEBUG] Tokenizer entrenado: vocab_size={len(self.tokenizer.word_index)}")
        elif not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            # Si no hay tokenizer entrenado y no estamos entrenando, error
            print("‚ùå [DEBUG] Error: tokenizer no est√° entrenado")
            raise ValueError("El tokenizer no est√° entrenado. Debe entrenar el modelo primero.")
        else:
            print(f"üîç [DEBUG] Usando tokenizer existente: vocab_size={len(self.tokenizer.word_index)}")
        
        print("üîç [DEBUG] Convirtiendo textos a secuencias...")
        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)
        print(f"üîç [DEBUG] Secuencias creadas: {[seq[:5] for seq in sequences[:3]]}")
        
        # Asegurar que todas las secuencias tengan al menos un elemento (OOV token)
        # Si una secuencia est√° vac√≠a, agregar el token OOV (√≠ndice 1 generalmente)
        sequences = [seq if seq else [1] for seq in sequences]
        print(f"üîç [DEBUG] Secuencias despu√©s de OOV: {[seq[:5] for seq in sequences[:3]]}")
        
        print(f"üîç [DEBUG] Haciendo padding: maxlen={self.max_len}")
        padded_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post', truncating='post')
        print(f"üîç [DEBUG] Secuencias con padding: shape={padded_sequences.shape}")
        
        if labels:
            print("üîç [DEBUG] Codificando etiquetas...")
            encoded_labels = self.label_encoder.fit_transform(labels)
            print(f"üîç [DEBUG] Etiquetas codificadas: {encoded_labels[:5]}")
            return padded_sequences, encoded_labels
        
        return padded_sequences
    
    def build_model(self, vocab_size: int, num_classes: int):
        """Construir red neuronal LSTM basada en texto para comentarios de hasta 25 palabras"""
        # Red neuronal LSTM real - suficiente capacidad para aprender patrones de texto
        model = Sequential([
            Embedding(vocab_size + 1, 24),  # Embedding layer (vectores de palabras)
            LSTM(16, dropout=0.2),        # LSTM layer (16 neuronas - aprende patrones de texto)
            Dense(8, activation='relu'),   # Dense layer (red neuronal)
            Dropout(0.2),
            Dense(num_classes, activation='softmax')  # Salida (probabilidades: positivo/negativo/neutral)
        ])
        
        # Compilar modelo neuronal
        model.compile(
            optimizer='adam',  # Optimizador de red neuronal
            loss='sparse_categorical_crossentropy',  # Funci√≥n de p√©rdida
            metrics=['accuracy']
        )
        
        return model
    
    def train(self, texts: List[str], labels: List[str], epochs=10, batch_size=32):
        """Entrenar modelo - Versi√≥n ULTRA-LIGERA para Render (512 MB limit)"""
        import tensorflow as tf
        import gc  # Para limpiar memoria
        
        print(f"üìä Preparando datos: {len(texts)} textos, {len(set(labels))} clases")
        X, y = self.prepare_data(texts, labels)
        
        # Limitar tama√±o de datos si es muy grande (para ahorrar memoria)
        max_samples = 150  # M√°ximo 150 muestras para entrenamiento (suficiente para comentarios)
        if len(X) > max_samples:
            print(f"‚ö†Ô∏è Reduciendo datos de {len(X)} a {max_samples} para ahorrar memoria...")
            X = X[:max_samples]
            y = y[:max_samples]
        
        # Si hay pocos datos, usar todos para entrenamiento (sin validaci√≥n)
        if len(X) < 50:
            X_train, y_train = X, y
            X_val, y_val = X, y
            use_validation = False
        else:
            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
            use_validation = True
        
        vocab_size = len(self.tokenizer.word_index)
        num_classes = len(self.label_encoder.classes_)
        print(f"üìä Vocabulario: {vocab_size} palabras, Clases: {num_classes}")
        print(f"üìä Datos entrenamiento: {len(X_train)}, Validaci√≥n: {len(X_val) if use_validation else 'N/A'}")
        
        # Limpiar memoria antes de construir modelo
        gc.collect()
        
        self.model = self.build_model(vocab_size, num_classes)
        
        print(f"üöÄ Iniciando entrenamiento: {epochs} √©pocas, batch_size={batch_size}")
        
        # Entrenar con batch size peque√±o para usar menos memoria
        fit_kwargs = {
            'epochs': epochs,
            'batch_size': batch_size,
            'verbose': 0,  # Sin logs para acelerar y ahorrar memoria
        }
        
        if use_validation:
            fit_kwargs['validation_data'] = (X_val, y_val)
            history = self.model.fit(X_train, y_train, **fit_kwargs)
            # Evaluar modelo
            val_loss, val_accuracy = self.model.evaluate(X_val, y_val, verbose=0)
            print(f"‚úÖ Entrenamiento completado - Precisi√≥n validaci√≥n: {val_accuracy:.2%}")
        else:
            history = self.model.fit(X_train, y_train, **fit_kwargs)
            print(f"‚úÖ Entrenamiento completado (sin validaci√≥n por datos limitados)")
        
        # Limpiar memoria despu√©s de entrenar
        gc.collect()
        
        self.is_trained = True
        return history
    
    def predict(self, texts: List[str]) -> List[Dict]:
        """Predecir sentimiento usando red neuronal LSTM"""
        print(f"üîç [DEBUG] predict() llamado con {len(texts)} texto(s)")
        
        # Validar que el modelo est√© completamente entrenado y listo
        print(f"üîç [DEBUG] Validando modelo: is_trained={self.is_trained}, model={self.model is not None}")
        if not self.is_trained:
            print("‚ùå [DEBUG] Error: modelo no est√° entrenado")
            raise ValueError(
                "El modelo de red neuronal no est√° entrenado. "
                "El modelo se est√° cargando o entrenando. Por favor, espera unos momentos."
            )
        
        if not self.model:
            print("‚ùå [DEBUG] Error: modelo no est√° inicializado")
            raise ValueError(
                "El modelo de red neuronal no est√° inicializado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        # Validar que el tokenizer est√© entrenado
        print(f"üîç [DEBUG] Validando tokenizer: has word_index={hasattr(self.tokenizer, 'word_index')}")
        if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            print("‚ùå [DEBUG] Error: tokenizer no tiene word_index")
            raise ValueError(
                "El tokenizer del modelo no est√° entrenado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        # Validar que el label encoder est√© entrenado
        print(f"üîç [DEBUG] Validando label_encoder: has classes={hasattr(self.label_encoder, 'classes_')}")
        if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
            print("‚ùå [DEBUG] Error: label_encoder no tiene classes")
            raise ValueError(
                "El label encoder del modelo no est√° entrenado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        if not texts:
            print("‚ùå [DEBUG] Error: lista de textos vac√≠a")
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        try:
            print(f"üîç [DEBUG] Preparando datos para {len(texts)} texto(s)...")
            # Preparar datos para predicci√≥n
            X = self.prepare_data(texts)
            print(f"üîç [DEBUG] Datos preparados: shape={X.shape}")
            
            # Verificar que tenemos datos v√°lidos
            if X.shape[0] == 0:
                print("‚ùå [DEBUG] Error: X.shape[0] == 0")
                raise ValueError("No se pudieron preparar los datos para predicci√≥n")
            
            print("üîç [DEBUG] Haciendo predicci√≥n con modelo neuronal...")
            # Hacer predicci√≥n con la red neuronal
            predictions = self.model.predict(X, verbose=0)
            print(f"üîç [DEBUG] Predicciones recibidas: shape={predictions.shape if predictions is not None else None}")
            
            # Validar predicciones
            if predictions is None:
                print("‚ùå [DEBUG] Error: predictions es None")
                raise ValueError("El modelo no devolvi√≥ predicciones (None)")
            
            if len(predictions) == 0:
                print("‚ùå [DEBUG] Error: predictions est√° vac√≠o")
                raise ValueError("El modelo no devolvi√≥ predicciones (vac√≠o)")
            
            print(f"üîç [DEBUG] Procesando predicciones...")
            # Procesar predicciones de la red neuronal
            predicted_classes = np.argmax(predictions, axis=1)
            print(f"üîç [DEBUG] predicted_classes: {predicted_classes}")
            
            predicted_labels = self.label_encoder.inverse_transform(predicted_classes)
            print(f"üîç [DEBUG] predicted_labels: {predicted_labels}")
            
            confidence = np.max(predictions, axis=1)
            print(f"üîç [DEBUG] confidence: {confidence}")
            
            results = []
            for i, text in enumerate(texts):
                if i >= len(predicted_labels):
                    print(f"‚ö†Ô∏è [DEBUG] Advertencia: √≠ndice {i} fuera de rango para predicciones")
                    label = 'neutral'
                    score = 0.5
                else:
                    label = predicted_labels[i]
                    score = float(confidence[i])
                
                if label == 'positivo':
                    sentiment = 'positivo'
                    emoji = 'üü¢'
                    score_value = score
                elif label == 'negativo':
                    sentiment = 'negativo'
                    emoji = 'üî¥'
                    score_value = -score
                else:
                    sentiment = 'neutral'
                    emoji = 'üü°'
                    score_value = 0.0
                
                results.append({
                    'text': text,
                    'sentiment': sentiment,
                    'score': round(score_value, 3),
                    'emoji': emoji,
                    'confidence': round(score, 3)
                })
            
            print(f"‚úÖ [DEBUG] Predicci√≥n completada: {len(results)} resultado(s)")
            return results
            
        except ValueError as e:
            # Re-lanzar ValueError con mensaje claro
            error_msg = str(e)
            print(f"‚ùå [DEBUG] ValueError en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"Error en predicci√≥n de red neuronal: {str(e)}"
            print(f"‚ùå [DEBUG] Exception en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
    
    def predict_single(self, text: str) -> Dict:
        """Predecir sentimiento de un solo texto"""
        print(f"üîç [DEBUG] predict_single() llamado con texto: '{text[:50]}...'")
        try:
            results = self.predict([text])
            if not results or len(results) == 0:
                print("‚ùå [DEBUG] Error: predict() no devolvi√≥ resultados")
                raise ValueError("No se obtuvieron resultados de la predicci√≥n")
            result = results[0]
            print(f"üîç [DEBUG] predict_single() resultado: sentiment={result.get('sentiment')}, score={result.get('score')}")
            return result
        except Exception as e:
            print(f"‚ùå [DEBUG] Error en predict_single: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def load_model(self, model_path: str = 'app/ml_models/sentiment_model.h5'):
        """Cargar modelo pre-entrenado"""
        # En Render, el sistema de archivos es ef√≠mero, as√≠ que siempre creamos el modelo en memoria
        # pero solo lo entrenamos una vez por instancia de la aplicaci√≥n
        
        # Asegurar que el directorio existe
        model_dir = os.path.dirname(model_path)
        if not os.path.exists(model_dir):
            os.makedirs(model_dir, exist_ok=True)
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        # Intentar cargar modelo existente (puede no existir en Render)
        if os.path.exists(model_path) and os.path.exists(tokenizer_path) and os.path.exists(label_encoder_path):
            try:
                print("üîÑ Cargando modelo de red neuronal pre-entrenado...")
                # Intentar cargar con compile=True primero (m√°s seguro)
                try:
                    self.model = load_model(model_path)
                    print("‚úÖ Modelo cargado con compilaci√≥n autom√°tica")
                except Exception as compile_error:
                    print(f"‚ö†Ô∏è Error al cargar con compilaci√≥n autom√°tica: {compile_error}")
                    print("üîÑ Intentando cargar sin compilaci√≥n y recompilando manualmente...")
                    self.model = load_model(model_path, compile=False)
                    # Recompilar el modelo
                    self.model.compile(
                        optimizer='adam',
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy']
                    )
                    print("‚úÖ Modelo recompilado correctamente")
                
                # Cargar tokenizer y label encoder
                with open(tokenizer_path, 'rb') as f:
                    self.tokenizer = pickle.load(f)
                with open(label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                
                # Verificar que el modelo est√° correctamente cargado
                if self.model is None:
                    raise ValueError("El modelo no se carg√≥ correctamente")
                if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
                    raise ValueError("El tokenizer no se carg√≥ correctamente")
                if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                    raise ValueError("El label encoder no se carg√≥ correctamente")
                
                self.is_trained = True
                
                # Validaci√≥n final: asegurar que el modelo puede hacer una predicci√≥n de prueba
                print("üîç [DEBUG] Validando modelo con predicci√≥n de prueba...")
                try:
                    # Hacer una predicci√≥n de prueba para validar que el modelo funciona
                    test_text = "excelente"
                    print(f"üîç [DEBUG] Texto de prueba: '{test_text}'")
                    test_X = self.prepare_data([test_text])
                    print(f"üîç [DEBUG] Datos de prueba preparados: shape={test_X.shape}")
                    test_pred = self.model.predict(test_X, verbose=0)
                    print(f"üîç [DEBUG] Predicci√≥n de prueba: {test_pred}")
                    if test_pred is None or len(test_pred) == 0:
                        raise ValueError("El modelo no puede hacer predicciones v√°lidas")
                    print("‚úÖ [DEBUG] Modelo validado correctamente con predicci√≥n de prueba")
                except Exception as e:
                    print(f"‚ö†Ô∏è [DEBUG] Error al validar modelo: {e}")
                    import traceback
                    traceback.print_exc()
                    # Si falla la validaci√≥n, marcar como no entrenado
                    self.is_trained = False
                    raise ValueError(f"El modelo no est√° funcionando correctamente: {str(e)}")
                
                print("‚úÖ Modelo de red neuronal cargado y verificado correctamente")
                return
            except Exception as e:
                print(f"‚ö†Ô∏è Error al cargar modelo pre-entrenado: {e}")
                import traceback
                traceback.print_exc()
                print("üîÑ Se crear√° un nuevo modelo...")
                # Limpiar archivos corruptos si existen
                try:
                    if os.path.exists(model_path):
                        os.remove(model_path)
                    if os.path.exists(tokenizer_path):
                        os.remove(tokenizer_path)
                    if os.path.exists(label_encoder_path):
                        os.remove(label_encoder_path)
                except:
                    pass
        
        # Si no existe o fall√≥ cargar, crear y entrenar modelo
        print("üîÑ Creando y entrenando modelo de red neuronal (versi√≥n r√°pida, ~10-20 segundos)...")
        print("üîç [DEBUG] Iniciando _create_pretrained_model()...")
        try:
            self._create_pretrained_model()
            print("‚úÖ Modelo de red neuronal entrenado y guardado correctamente")
            
            # Validar que el modelo est√© completamente listo despu√©s del entrenamiento
            print("üîç [DEBUG] Validando modelo despu√©s del entrenamiento...")
            if not self.is_trained:
                raise ValueError("El modelo no se marc√≥ como entrenado despu√©s de _create_pretrained_model()")
            if not self.model:
                raise ValueError("El modelo no se cre√≥ despu√©s de _create_pretrained_model()")
            if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
                raise ValueError("El tokenizer no se entren√≥ correctamente")
            if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                raise ValueError("El label encoder no se entren√≥ correctamente")
            
            print("‚úÖ [DEBUG] Modelo completamente validado despu√©s del entrenamiento")
        except Exception as e:
            print(f"‚ùå Error al crear modelo de red neuronal: {e}")
            import traceback
            traceback.print_exc()
            self.is_trained = False
            raise
    
    def _create_pretrained_model(self):
        """Entrenar red neuronal LSTM con comentarios de hasta 25 palabras"""
        print("üîç [DEBUG] _create_pretrained_model() iniciado")
        # Datos de entrenamiento con comentarios completos (hasta 25 palabras)
        # Incluir frases cortas Y comentarios completos para mejor aprendizaje
        
        # Comentarios POSITIVOS (hasta 25 palabras)
        positive_texts = [
            # Frases cortas
            "excelente producto muy bueno", "me encanta este servicio", "muy satisfecho",
            "recomiendo totalmente", "calidad superior", "atenci√≥n perfecta",
            "super contento", "vale la pena", "muy recomendado", "incre√≠ble experiencia",
            "excelente servicio", "muy buena calidad", "excelente atenci√≥n", 
            "producto genial", "muy bien hecho", "s√∫per recomendable",
            
            # Comentarios completos (10-25 palabras)
            "excelente servicio al cliente muy atento y profesional la atenci√≥n fue r√°pida y eficiente",
            "me encant√≥ este producto la calidad es superior y el precio es muy razonable lo recomiendo totalmente",
            "muy buena experiencia de compra el producto lleg√≥ r√°pido y en perfecto estado estoy muy satisfecho",
            "servicio impecable desde el primer contacto hasta la entrega todo fue perfecto muy recomendado",
            "calidad excelente el producto super√≥ mis expectativas y el servicio fue muy profesional y amable",
            "incre√≠ble experiencia el producto es de muy buena calidad y la atenci√≥n al cliente fue excepcional",
            "muy contento con la compra el servicio fue r√°pido y el producto es de excelente calidad",
            "recomiendo totalmente este producto la calidad es superior y el precio es muy justo",
        ]
        
        # Comentarios NEGATIVOS (hasta 25 palabras)
        negative_texts = [
            # Frases cortas
            "p√©simo servicio muy malo", "no recomiendo para nada", "calidad terrible",
            "muy decepcionado", "atenci√≥n horrible", "lento e ineficiente", "no vale la pena",
            "muy insatisfecho", "problema grave", "no cumpli√≥ expectativas", "servicio p√©simo",
            "mal servicio", "muy mala calidad", "no funciona bien", "muy decepcionante",
            
            # Comentarios completos (10-25 palabras)
            "p√©simo servicio al cliente muy lento y desatento la atenci√≥n fue horrible y no resolvieron mi problema",
            "muy decepcionado con este producto la calidad es terrible y no funciona como se esperaba no lo recomiendo",
            "servicio muy malo el producto lleg√≥ tarde y en mal estado estoy muy insatisfecho con la compra",
            "no recomiendo para nada este producto tiene muchos defectos y el servicio al cliente es p√©simo",
            "muy mala experiencia el producto no cumple con lo prometido y la atenci√≥n fue horrible",
            "calidad terrible el producto se rompi√≥ al poco tiempo y el servicio no respondi√≥ a mis quejas",
            "problema grave con este producto no funciona correctamente y el servicio al cliente fue ineficiente",
            "muy insatisfecho con la compra el producto es de mala calidad y el servicio fue p√©simo",
        ]
        
        # Comentarios NEUTRALES (hasta 25 palabras)
        neutral_texts = [
            # Frases cortas
            "producto regular", "ni bueno ni malo", "aceptable", "normal", "sin comentarios",
            "b√°sico", "est√°ndar", "cumple su funci√≥n", "nada especial", "producto com√∫n",
            "servicio est√°ndar", "normal como cualquier otro", "ni destacable ni malo",
            "producto promedio", "servicio b√°sico",
            
            # Comentarios completos (10-25 palabras)
            "producto regular que cumple su funci√≥n b√°sica nada especial pero tampoco tiene problemas mayores",
            "servicio est√°ndar normal como cualquier otro no destac√≥ ni positivo ni negativo simplemente aceptable",
            "producto promedio que funciona como se espera sin nada que destacar pero tampoco con problemas",
            "experiencia normal el producto es b√°sico y cumple su funci√≥n sin sorpresas positivas ni negativas",
            "servicio b√°sico que funciona correctamente sin problemas pero tampoco con caracter√≠sticas especiales",
            "producto com√∫n que cumple con lo m√≠nimo esperado ni bueno ni malo simplemente aceptable",
        ]
        
        texts = positive_texts + negative_texts + neutral_texts
        labels = (['positivo'] * len(positive_texts) + 
                 ['negativo'] * len(negative_texts) + 
                 ['neutral'] * len(neutral_texts))
        
        print("üîÑ Entrenando red neuronal LSTM para comentarios de hasta 25 palabras...")
        print(f"üìä Total de textos: {len(texts)}, Clases: {len(set(labels))}")
        print(f"üîç [DEBUG] Textos positivos: {len(positive_texts)}, negativos: {len(negative_texts)}, neutrales: {len(neutral_texts)}")
        
        # Entrenamiento con m√°s √©pocas para mejor aprendizaje de comentarios completos
        print("üîç [DEBUG] Iniciando entrenamiento...")
        try:
            self.train(texts, labels, epochs=5, batch_size=12)  # 5 √©pocas para mejor aprendizaje
            print("‚úÖ [DEBUG] Entrenamiento completado")
            
            # Validar que el modelo est√° entrenado
            if not self.is_trained:
                raise ValueError("El modelo no se marc√≥ como entrenado despu√©s del entrenamiento")
            if not self.model:
                raise ValueError("El modelo no existe despu√©s del entrenamiento")
            
            print("üîç [DEBUG] Guardando modelo...")
            self.save_model()
            print("‚úÖ [DEBUG] Modelo guardado correctamente")
            
            # Validaci√≥n final: hacer una predicci√≥n de prueba
            print("üîç [DEBUG] Haciendo predicci√≥n de prueba despu√©s del entrenamiento...")
            test_result = self.predict_single("excelente servicio")
            print(f"üîç [DEBUG] Predicci√≥n de prueba: {test_result}")
            
        except Exception as e:
            print(f"‚ùå [DEBUG] Error en _create_pretrained_model: {str(e)}")
            import traceback
            traceback.print_exc()
            self.is_trained = False
            raise
        
        print("‚úÖ Red neuronal LSTM entrenada correctamente (soporta comentarios de hasta 25 palabras)")
    
    def save_model(self, model_path: str = 'app/ml_models/sentiment_model.h5'):
        """Guardar modelo"""
        model_dir = os.path.dirname(model_path)
        os.makedirs(model_dir, exist_ok=True)
        
        if self.model:
            self.model.save(model_path)
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(self.tokenizer, f)
        with open(label_encoder_path, 'wb') as f:
            pickle.dump(self.label_encoder, f)

