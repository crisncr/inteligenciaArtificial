import re
import numpy as np
from typing import Dict, List, Tuple
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback
import time

class TrainingProgressCallback(Callback):
    """Callback para monitorear el progreso del entrenamiento"""
    def __init__(self):
        self.start_time = None
        self.epoch_times = []
        import sys
        self.stdout = sys.stdout
    
    def _print_and_flush(self, message):
        """Imprimir y hacer flush inmediatamente"""
        print(message)
        self.stdout.flush()
    
    def on_train_begin(self, logs=None):
        self.start_time = time.time()
        self._print_and_flush(f"‚è±Ô∏è [DEBUG] Entrenamiento comenz√≥ a las {time.strftime('%H:%M:%S')}")
        self._print_and_flush(f"üîç [DEBUG] Callback on_train_begin ejecutado correctamente")
    
    def on_epoch_begin(self, epoch, logs=None):
        epoch_start = time.time()
        self._print_and_flush(f"üîÑ [DEBUG] √âpoca {epoch + 1} comenzando a las {time.strftime('%H:%M:%S')}...")
        self.current_epoch_start = epoch_start
    
    def on_batch_begin(self, batch, logs=None):
        # Log cada 5 batches para no saturar, pero ver progreso
        if batch % 5 == 0 or batch == 0:
            self._print_and_flush(f"üîç [DEBUG] Batch {batch} comenzando...")
    
    def on_batch_end(self, batch, logs=None):
        # Log cada 5 batches para no saturar, pero ver progreso
        if batch % 5 == 0 or batch == 0:
            loss = logs.get('loss', 'N/A')
            acc = logs.get('accuracy', 'N/A')
            self._print_and_flush(f"üîç [DEBUG] Batch {batch} completado - loss: {loss}, accuracy: {acc}")
    
    def on_epoch_end(self, epoch, logs=None):
        epoch_time = time.time() - self.current_epoch_start
        self.epoch_times.append(epoch_time)
        loss = logs.get('loss', 'N/A')
        accuracy = logs.get('accuracy', 'N/A')
        val_loss = logs.get('val_loss', 'N/A')
        val_accuracy = logs.get('val_accuracy', 'N/A')
        self._print_and_flush(f"‚úÖ [DEBUG] √âpoca {epoch + 1} completada en {epoch_time:.2f}s - loss: {loss:.4f}, accuracy: {accuracy:.4f}, val_loss: {val_loss}, val_accuracy: {val_accuracy}")
    
    def on_train_end(self, logs=None):
        total_time = time.time() - self.start_time
        self._print_and_flush(f"‚è±Ô∏è [DEBUG] Entrenamiento terminado en {total_time:.2f}s total")
        if self.epoch_times:
            avg_time = sum(self.epoch_times) / len(self.epoch_times)
            self._print_and_flush(f"üìä [DEBUG] Tiempo promedio por √©poca: {avg_time:.2f}s")
        else:
            self._print_and_flush(f"‚ö†Ô∏è [DEBUG] No se registraron √©pocas completadas")

class SentimentNeuralNetwork:
    def __init__(self, max_words=800, max_len=35):
        # Red neuronal LSTM basada en texto - Soporta comentarios de hasta 25 palabras
        # max_words: 800 (vocabulario suficiente para comentarios)
        # max_len: 35 (soporta c√≥modamente hasta 25 palabras)
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
        self.label_encoder = LabelEncoder()
        self.model = None
        self.is_trained = False
        
    def clean_text(self, text: str) -> str:
        """Limpieza de texto mejorada con normalizaci√≥n"""
        if not text:
            return ""
        
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Normalizar tildes y caracteres especiales (esto ayuda a que "atenci√≥n" y "atencion" se traten igual)
        # Reemplazar tildes por versiones sin tilde para normalizar
        replacements = {
            '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
            '√±': 'n', '√º': 'u'
        }
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # Eliminar URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Eliminar menciones y hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        
        # Eliminar caracteres especiales excepto letras, n√∫meros y espacios
        text = re.sub(r'[^a-z0-9\s]', ' ', text)
        
        # Eliminar espacios m√∫ltiples
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def prepare_data(self, texts: List[str], labels: List[str] = None) -> Tuple:
        """Preparar datos para entrenamiento o predicci√≥n"""
        print(f"üîç [DEBUG] prepare_data() llamado con {len(texts)} texto(s), labels={labels is not None}")
        
        if not texts:
            print("‚ùå [DEBUG] Error: lista de textos vac√≠a en prepare_data")
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        # Limpiar textos
        print("üîç [DEBUG] Limpiando textos...")
        cleaned_texts = [self.clean_text(text) if text else "" for text in texts]
        print(f"üîç [DEBUG] Textos limpiados: {[t[:30] + '...' if len(t) > 30 else t for t in cleaned_texts[:3]]}")
        
        # Tokenizar
        if labels:
            # Si hay etiquetas, estamos entrenando, ajustar tokenizer
            print("üîç [DEBUG] Entrenando tokenizer...")
            self.tokenizer.fit_on_texts(cleaned_texts)
            print(f"üîç [DEBUG] Tokenizer entrenado: vocab_size={len(self.tokenizer.word_index)}")
        elif not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            # Si no hay tokenizer entrenado y no estamos entrenando, error
            print("‚ùå [DEBUG] Error: tokenizer no est√° entrenado")
            raise ValueError("El tokenizer no est√° entrenado. Debe entrenar el modelo primero.")
        else:
            print(f"üîç [DEBUG] Usando tokenizer existente: vocab_size={len(self.tokenizer.word_index)}")
        
        print("üîç [DEBUG] Convirtiendo textos a secuencias...")
        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)
        print(f"üîç [DEBUG] Secuencias creadas: {[seq[:5] for seq in sequences[:3]]}")
        
        # Asegurar que todas las secuencias tengan al menos un elemento (OOV token)
        # Si una secuencia est√° vac√≠a, agregar el token OOV (√≠ndice 1 generalmente)
        sequences = [seq if seq else [1] for seq in sequences]
        print(f"üîç [DEBUG] Secuencias despu√©s de OOV: {[seq[:5] for seq in sequences[:3]]}")
        
        print(f"üîç [DEBUG] Haciendo padding: maxlen={self.max_len}")
        padded_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post', truncating='post')
        print(f"üîç [DEBUG] Secuencias con padding: shape={padded_sequences.shape}")
        
        if labels:
            print("üîç [DEBUG] Codificando etiquetas...")
            encoded_labels = self.label_encoder.fit_transform(labels)
            # Mostrar distribuci√≥n completa de etiquetas codificadas
            unique_encoded, counts_encoded = np.unique(encoded_labels, return_counts=True)
            label_names_encoded = self.label_encoder.inverse_transform(unique_encoded)
            print(f"üîç [DEBUG] Distribuci√≥n de etiquetas codificadas:")
            for label_name, label_code, count in zip(label_names_encoded, unique_encoded, counts_encoded):
                print(f"   - {label_name} (c√≥digo {label_code}): {count} muestras")
            print(f"üîç [DEBUG] Primeras 10 etiquetas codificadas: {encoded_labels[:10]}")
            return padded_sequences, encoded_labels
        
        return padded_sequences
    
    def build_model(self, vocab_size: int, num_classes: int):
        """Construir red neuronal LSTM basada en texto para comentarios de hasta 25 palabras"""
        print(f"üîç [DEBUG] Construyendo modelo: vocab_size={vocab_size}, num_classes={num_classes}")
        print(f"üîç [DEBUG] Par√°metros del modelo: max_words={self.max_words}, max_len={self.max_len}")
        
        # Red neuronal LSTM optimizada para entrenamiento R√ÅPIDO pero efectivo
        # Modelo balanceado: suficientemente grande para aprender, pero peque√±o para entrenar r√°pido
        from tensorflow.keras.initializers import GlorotUniform
        
        model = Sequential([
            Embedding(vocab_size + 1, 8, embeddings_initializer=GlorotUniform()),  # 8 dimensiones (balance entre velocidad y capacidad)
            LSTM(4, dropout=0.1, kernel_initializer=GlorotUniform()),        # 4 unidades (r√°pido pero efectivo)
            Dense(4, activation='relu', kernel_initializer=GlorotUniform()),   # 4 unidades
            Dense(num_classes, activation='softmax', kernel_initializer=GlorotUniform())  # Salida
        ])
        
        print(f"üîç [DEBUG] Modelo construido, compilando...")
        # Compilar modelo neuronal con learning rate optimizado
        from tensorflow.keras.optimizers import Adam
        optimizer = Adam(learning_rate=0.005)  # Learning rate balanceado (0.005) para aprender bien sin overshooting
        model.compile(
            optimizer=optimizer,  # Optimizador con learning rate configurado
            loss='sparse_categorical_crossentropy',  # Funci√≥n de p√©rdida
            metrics=['accuracy'],
            run_eagerly=True  # Ejecutar en modo eager para evitar bloqueos durante compilaci√≥n
        )
        
        # NO contar par√°metros aqu√≠ - el modelo a√∫n no est√° "built"
        # Los par√°metros se contar√°n despu√©s del primer fit() cuando el modelo se construya autom√°ticamente
        print(f"üîç [DEBUG] Modelo compilado correctamente (run_eagerly=True)")
        
        return model
    
    def train(self, texts: List[str], labels: List[str], epochs=10, batch_size=32):
        """Entrenar modelo - Versi√≥n ULTRA-LIGERA para Render (512 MB limit)"""
        import tensorflow as tf
        import gc  # Para limpiar memoria
        
        print(f"üìä Preparando datos: {len(texts)} textos, {len(set(labels))} clases")
        X, y = self.prepare_data(texts, labels)
        
        # Mostrar distribuci√≥n de etiquetas ANTES de reducir
        unique_labels, counts = np.unique(y, return_counts=True)
        label_names = self.label_encoder.inverse_transform(unique_labels)
        print(f"üîç [DEBUG] Distribuci√≥n de etiquetas ANTES de reducir:")
        for label_name, count in zip(label_names, counts):
            print(f"   - {label_name}: {count} muestras")
        
        # NO reducir datos - usar TODOS para mejor aprendizaje
        # Con modelo m√°s peque√±o y menos √©pocas, podemos usar m√°s datos sin tardar mucho
        max_samples = 1000  # Usar todos los datos disponibles (no reducir)
        if len(X) > max_samples:
            print(f"‚ö†Ô∏è Reduciendo datos de {len(X)} a {max_samples} para ahorrar memoria y velocidad...")
            
            # CR√çTICO: Mezclar datos ANTES de reducir para mantener balance de clases
            # Esto asegura que no tomemos solo los primeros elementos que pueden ser de la misma clase
            indices = np.arange(len(X))
            np.random.seed(42)  # Semilla fija para reproducibilidad
            np.random.shuffle(indices)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            # Intentar mantener balance de clases al reducir
            # Asegurar que haya al menos algunas muestras de cada clase
            unique_labels_all = np.unique(y_shuffled)
            num_classes_available = len(unique_labels_all)
            samples_per_class = max_samples // num_classes_available
            min_samples_per_class = max(1, samples_per_class - 1)  # Al menos 1 por clase
            
            print(f"üîç [DEBUG] Intentando balancear: {min_samples_per_class} muestras m√≠nimas por clase de {num_classes_available} clases")
            
            # Recopilar muestras balanceadas
            X_balanced = []
            y_balanced = []
            samples_taken_per_class = {int(label): 0 for label in unique_labels_all}
            used_indices = set()
            
            # Primero, tomar al menos min_samples_per_class de cada clase
            for label in unique_labels_all:
                label_int = int(label)
                label_indices = np.where(y_shuffled == label)[0]
                np.random.shuffle(label_indices)
                
                for idx in label_indices[:min_samples_per_class]:
                    if len(X_balanced) >= max_samples:
                        break
                    if idx not in used_indices:
                        X_balanced.append(X_shuffled[idx])
                        y_balanced.append(y_shuffled[idx])
                        used_indices.add(idx)
                        samples_taken_per_class[label_int] += 1
                
                if len(X_balanced) >= max_samples:
                    break
            
            # Si a√∫n hay espacio, tomar muestras adicionales de manera aleatoria
            remaining_indices = [i for i in range(len(X_shuffled)) if i not in used_indices]
            np.random.shuffle(remaining_indices)
            
            for idx in remaining_indices:
                if len(X_balanced) >= max_samples:
                    break
                X_balanced.append(X_shuffled[idx])
                y_balanced.append(y_shuffled[idx])
                used_indices.add(idx)
            
            X = np.array(X_balanced)
            y = np.array(y_balanced)
            
            # Validar balance de clases DESPU√âS de reducir
            unique_labels_reduced, counts_reduced = np.unique(y, return_counts=True)
            label_names_reduced = self.label_encoder.inverse_transform(unique_labels_reduced)
            print(f"üîç [DEBUG] Distribuci√≥n de etiquetas DESPU√âS de reducir (balanceado):")
            for label_name, count in zip(label_names_reduced, counts_reduced):
                print(f"   - {label_name}: {count} muestras")
        
        # Si hay pocos datos, usar todos para entrenamiento (sin validaci√≥n)
        if len(X) < 50:
            X_train, y_train = X, y
            X_val, y_val = X, y
            use_validation = False
        else:
            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
            use_validation = True
        
        vocab_size = len(self.tokenizer.word_index)
        num_classes = len(self.label_encoder.classes_)
        print(f"üìä Vocabulario: {vocab_size} palabras, Clases: {num_classes}")
        print(f"üìä Datos entrenamiento: {len(X_train)}, Validaci√≥n: {len(X_val) if use_validation else 'N/A'}")
        
        # Validar balance de clases en datos de entrenamiento
        unique_labels_train, counts_train = np.unique(y_train, return_counts=True)
        label_names_train = self.label_encoder.inverse_transform(unique_labels_train)
        print(f"üîç [DEBUG] Distribuci√≥n de etiquetas en datos de ENTRENAMIENTO:")
        for label_name, count in zip(label_names_train, counts_train):
            print(f"   - {label_name}: {count} muestras")
        
        # Verificar que haya al menos una muestra de cada clase en entrenamiento
        if len(unique_labels_train) < num_classes:
            print(f"‚ö†Ô∏è [DEBUG] ADVERTENCIA: Solo hay {len(unique_labels_train)} clases en entrenamiento, esperadas {num_classes}")
            print(f"‚ö†Ô∏è [DEBUG] Esto puede afectar la precisi√≥n del modelo")
        else:
            print(f"‚úÖ [DEBUG] Todas las clases ({num_classes}) est√°n representadas en los datos de entrenamiento")
        
        # Limpiar memoria antes de construir modelo
        print("üîç [DEBUG] Limpiando memoria antes de construir modelo...")
        gc.collect()
        
        print("üîç [DEBUG] Construyendo modelo...")
        build_start = time.time()
        self.model = self.build_model(vocab_size, num_classes)
        build_time = time.time() - build_start
        print(f"‚úÖ [DEBUG] Modelo construido en {build_time:.2f}s")
        
        # Optimizar √©pocas: suficiente para aprender, pero r√°pido
        actual_epochs = min(epochs, 2)  # Solo 2 √©pocas para entrenamiento r√°pido (con m√°s datos y mejor LR)
        # Batch size debe ser menor o igual al n√∫mero de muestras
        # Si hay 15 muestras, usar batch_size=15 (entrenar todas a la vez es m√°s r√°pido)
        actual_batch_size = min(batch_size, len(X_train))  # No puede ser mayor que las muestras disponibles
        if actual_batch_size > len(X_train):
            actual_batch_size = len(X_train)  # Usar todas las muestras en un solo batch
        print(f"üîç [DEBUG] Batch size ajustado: {actual_batch_size} (muestras disponibles: {len(X_train)})")
        
        print(f"üöÄ Iniciando entrenamiento: {actual_epochs} √©pocas (reducido de {epochs}), batch_size={actual_batch_size} (ajustado de {batch_size})")
        print(f"üìä Datos de entrenamiento: {len(X_train)} muestras")
        print(f"üìä Shape de X_train: {X_train.shape}, Shape de y_train: {y_train.shape}")
        
        # Crear callback de progreso
        progress_callback = TrainingProgressCallback()
        
        # Entrenar con batch size m√°s grande para m√°s velocidad
        # run_eagerly ya est√° configurado en compile() para evitar bloqueos
        fit_kwargs = {
            'epochs': actual_epochs,
            'batch_size': actual_batch_size,
            'verbose': 0,  # Sin logs de TensorFlow (usamos nuestro callback)
            'callbacks': [progress_callback]  # Agregar callback de progreso
        }
        
        try:
            if use_validation:
                fit_kwargs['validation_data'] = (X_val, y_val)
                print("üîç [DEBUG] Llamando a model.fit() con validaci√≥n...")
                print(f"üîç [DEBUG] X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
                print(f"üîç [DEBUG] X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")
                print(f"üîç [DEBUG] Callback creado: {progress_callback}")
                print(f"üîç [DEBUG] fit_kwargs: {fit_kwargs}")
                
                # Flush stdout para asegurar que los logs se muestren
                import sys
                sys.stdout.flush()
                
                print("üöÄ [DEBUG] INICIANDO model.fit() CON VALIDACI√ìN AHORA...")
                sys.stdout.flush()
                
                fit_start = time.time()
                try:
                    history = self.model.fit(X_train, y_train, **fit_kwargs)
                    fit_time = time.time() - fit_start
                    print(f"‚úÖ [DEBUG] model.fit() completado en {fit_time:.2f}s")
                except Exception as fit_error:
                    fit_time = time.time() - fit_start
                    print(f"‚ùå [DEBUG] ERROR en model.fit() despu√©s de {fit_time:.2f}s: {str(fit_error)}")
                    import traceback
                    traceback.print_exc()
                    raise
                
                # Ahora s√≠ podemos contar los par√°metros (el modelo ya est√° "built" despu√©s del fit)
                try:
                    total_params = self.model.count_params()
                    print(f"üìä [DEBUG] Modelo entrenado - Total de par√°metros: {total_params:,}")
                except Exception as e:
                    print(f"‚ö†Ô∏è [DEBUG] No se pudo contar par√°metros: {e}")
                
                # Evaluar modelo
                print("üîç [DEBUG] Evaluando modelo...")
                eval_start = time.time()
                val_loss, val_accuracy = self.model.evaluate(X_val, y_val, verbose=0)
                eval_time = time.time() - eval_start
                print(f"‚úÖ [DEBUG] Evaluaci√≥n completada en {eval_time:.2f}s")
                print(f"‚úÖ Entrenamiento completado - Precisi√≥n validaci√≥n: {val_accuracy:.2%}")
            else:
                print("üîç [DEBUG] Llamando a model.fit() sin validaci√≥n...")
                print(f"üîç [DEBUG] X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
                print(f"üîç [DEBUG] Callback creado: {progress_callback}")
                print(f"üîç [DEBUG] fit_kwargs: {fit_kwargs}")
                print(f"üîç [DEBUG] Modelo antes de fit: {self.model}")
                print(f"üîç [DEBUG] Verificando que el modelo est√© compilado...")
                print(f"üîç [DEBUG] Optimizer: {self.model.optimizer}")
                
                # Flush stdout para asegurar que los logs se muestren
                import sys
                sys.stdout.flush()
                
                print("üöÄ [DEBUG] INICIANDO model.fit() AHORA...")
                print(f"üîç [DEBUG] Par√°metros: epochs={actual_epochs}, batch_size={actual_batch_size}, samples={len(X_train)}")
                sys.stdout.flush()
                
                fit_start = time.time()
                try:
                    # Agregar logging peri√≥dico durante el entrenamiento
                    print(f"üîç [DEBUG] Llamando a model.fit() - esto puede tomar 10-30 segundos...")
                    sys.stdout.flush()
                    
                    history = self.model.fit(X_train, y_train, **fit_kwargs)
                    fit_time = time.time() - fit_start
                    print(f"‚úÖ [DEBUG] model.fit() completado en {fit_time:.2f}s")
                    sys.stdout.flush()
                except Exception as fit_error:
                    fit_time = time.time() - fit_start
                    print(f"‚ùå [DEBUG] ERROR en model.fit() despu√©s de {fit_time:.2f}s: {str(fit_error)}")
                    print(f"üîç [DEBUG] Tipo de error: {type(fit_error).__name__}")
                    import traceback
                    traceback.print_exc()
                    sys.stdout.flush()
                    raise
                
                # Ahora s√≠ podemos contar los par√°metros (el modelo ya est√° "built" despu√©s del fit)
                try:
                    total_params = self.model.count_params()
                    print(f"üìä [DEBUG] Modelo entrenado - Total de par√°metros: {total_params:,}")
                except Exception as e:
                    print(f"‚ö†Ô∏è [DEBUG] No se pudo contar par√°metros: {e}")
                
                print(f"‚úÖ Entrenamiento completado (sin validaci√≥n por datos limitados)")
        except Exception as e:
            print(f"‚ùå [DEBUG] ERROR durante model.fit(): {str(e)}")
            import traceback
            traceback.print_exc()
            raise
        
        # Limpiar memoria despu√©s de entrenar
        print("üîç [DEBUG] Limpiando memoria despu√©s de entrenar...")
        gc.collect()
        
        # Validar que el modelo est√© correctamente entrenado
        print("üîç [DEBUG] Validando modelo despu√©s del entrenamiento...")
        if self.model is None:
            raise ValueError("El modelo no existe despu√©s del entrenamiento")
        
        print("üîç [DEBUG] Marcando modelo como entrenado...")
        self.is_trained = True
        print(f"‚úÖ [DEBUG] Modelo marcado como entrenado: is_trained={self.is_trained}")
        print(f"üîç [DEBUG] Modelo existe: {self.model is not None}")
        print(f"üîç [DEBUG] Tokenizer tiene word_index: {hasattr(self.tokenizer, 'word_index') and len(self.tokenizer.word_index) > 0}")
        print(f"üîç [DEBUG] Label encoder tiene classes: {hasattr(self.label_encoder, 'classes_') and len(self.label_encoder.classes_) > 0}")
        
        return history
    
    def predict(self, texts: List[str]) -> List[Dict]:
        """Predecir sentimiento usando red neuronal LSTM"""
        print(f"üîç [DEBUG] predict() llamado con {len(texts)} texto(s)")
        
        # Validar que el modelo est√© completamente entrenado y listo
        print(f"üîç [DEBUG] Validando modelo: is_trained={self.is_trained}, model={self.model is not None}")
        if not self.is_trained:
            print("‚ùå [DEBUG] Error: modelo no est√° entrenado")
            raise ValueError(
                "El modelo de red neuronal no est√° entrenado. "
                "El modelo se est√° cargando o entrenando. Por favor, espera unos momentos."
            )
        
        if not self.model:
            print("‚ùå [DEBUG] Error: modelo no est√° inicializado")
            raise ValueError(
                "El modelo de red neuronal no est√° inicializado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        # Validar que el tokenizer est√© entrenado
        print(f"üîç [DEBUG] Validando tokenizer: has word_index={hasattr(self.tokenizer, 'word_index')}")
        if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            print("‚ùå [DEBUG] Error: tokenizer no tiene word_index")
            raise ValueError(
                "El tokenizer del modelo no est√° entrenado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        # Validar que el label encoder est√© entrenado
        print(f"üîç [DEBUG] Validando label_encoder: has classes={hasattr(self.label_encoder, 'classes_')}")
        if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
            print("‚ùå [DEBUG] Error: label_encoder no tiene classes")
            raise ValueError(
                "El label encoder del modelo no est√° entrenado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        if not texts:
            print("‚ùå [DEBUG] Error: lista de textos vac√≠a")
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        try:
            print(f"üîç [DEBUG] Preparando datos para {len(texts)} texto(s)...")
            # Preparar datos para predicci√≥n
            X = self.prepare_data(texts)
            print(f"üîç [DEBUG] Datos preparados: shape={X.shape}")
            
            # Verificar que tenemos datos v√°lidos
            if X.shape[0] == 0:
                print("‚ùå [DEBUG] Error: X.shape[0] == 0")
                raise ValueError("No se pudieron preparar los datos para predicci√≥n")
            
            print("üîç [DEBUG] Haciendo predicci√≥n con modelo neuronal...")
            # Hacer predicci√≥n con la red neuronal
            predictions = self.model.predict(X, verbose=0)
            print(f"üîç [DEBUG] Predicciones recibidas: shape={predictions.shape if predictions is not None else None}")
            
            # Validar predicciones
            if predictions is None:
                print("‚ùå [DEBUG] Error: predictions es None")
                raise ValueError("El modelo no devolvi√≥ predicciones (None)")
            
            if len(predictions) == 0:
                print("‚ùå [DEBUG] Error: predictions est√° vac√≠o")
                raise ValueError("El modelo no devolvi√≥ predicciones (vac√≠o)")
            
            print(f"üîç [DEBUG] Procesando predicciones...")
            # Mostrar probabilidades completas para diagn√≥stico
            print(f"üîç [DEBUG] Probabilidades completas (primeras 3 predicciones):")
            for i in range(min(3, len(predictions))):
                probs = predictions[i]
                label_names = self.label_encoder.classes_
                print(f"   Predicci√≥n {i}: {dict(zip(label_names, probs))}")
            
            # Procesar predicciones de la red neuronal
            predicted_classes = np.argmax(predictions, axis=1)
            print(f"üîç [DEBUG] predicted_classes: {predicted_classes}")
            
            predicted_labels = self.label_encoder.inverse_transform(predicted_classes)
            print(f"üîç [DEBUG] predicted_labels: {predicted_labels}")
            
            confidence = np.max(predictions, axis=1)
            print(f"üîç [DEBUG] confidence: {confidence}")
            
            results = []
            for i, text in enumerate(texts):
                if i >= len(predicted_labels):
                    print(f"‚ö†Ô∏è [DEBUG] Advertencia: √≠ndice {i} fuera de rango para predicciones")
                    label = 'neutral'
                    score = 0.5
                else:
                    label = predicted_labels[i]
                    score = float(confidence[i])
                
                if label == 'positivo':
                    sentiment = 'positivo'
                    emoji = 'üü¢'
                    score_value = score
                elif label == 'negativo':
                    sentiment = 'negativo'
                    emoji = 'üî¥'
                    score_value = -score
                else:
                    sentiment = 'neutral'
                    emoji = 'üü°'
                    score_value = 0.0
                
                results.append({
                    'text': text,
                    'sentiment': sentiment,
                    'score': round(score_value, 3),
                    'emoji': emoji,
                    'confidence': round(score, 3)
                })
            
            print(f"‚úÖ [DEBUG] Predicci√≥n completada: {len(results)} resultado(s)")
            return results
            
        except ValueError as e:
            # Re-lanzar ValueError con mensaje claro
            error_msg = str(e)
            print(f"‚ùå [DEBUG] ValueError en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"Error en predicci√≥n de red neuronal: {str(e)}"
            print(f"‚ùå [DEBUG] Exception en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
    
    def predict_single(self, text: str) -> Dict:
        """Predecir sentimiento de un solo texto"""
        print(f"üîç [DEBUG] predict_single() llamado con texto: '{text[:50]}...'")
        try:
            results = self.predict([text])
            if not results or len(results) == 0:
                print("‚ùå [DEBUG] Error: predict() no devolvi√≥ resultados")
                raise ValueError("No se obtuvieron resultados de la predicci√≥n")
            result = results[0]
            print(f"üîç [DEBUG] predict_single() resultado: sentiment={result.get('sentiment')}, score={result.get('score')}")
            return result
        except Exception as e:
            print(f"‚ùå [DEBUG] Error en predict_single: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def load_model(self, model_path: str = 'app/ml_models/sentiment_model.h5'):
        """Cargar modelo pre-entrenado"""
        # En Render, el sistema de archivos es ef√≠mero, as√≠ que siempre creamos el modelo en memoria
        # pero solo lo entrenamos una vez por instancia de la aplicaci√≥n
        
        # Asegurar que el directorio existe
        model_dir = os.path.dirname(model_path)
        if not os.path.exists(model_dir):
            os.makedirs(model_dir, exist_ok=True)
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        # Intentar cargar modelo existente (puede no existir en Render)
        if os.path.exists(model_path) and os.path.exists(tokenizer_path) and os.path.exists(label_encoder_path):
            try:
                print("üîÑ Cargando modelo de red neuronal pre-entrenado...")
                # Intentar cargar con compile=True primero (m√°s seguro)
                try:
                    self.model = load_model(model_path)
                    print("‚úÖ Modelo cargado con compilaci√≥n autom√°tica")
                except Exception as compile_error:
                    print(f"‚ö†Ô∏è Error al cargar con compilaci√≥n autom√°tica: {compile_error}")
                    print("üîÑ Intentando cargar sin compilaci√≥n y recompilando manualmente...")
                    self.model = load_model(model_path, compile=False)
                    # Recompilar el modelo
                    self.model.compile(
                        optimizer='adam',
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy']
                    )
                    print("‚úÖ Modelo recompilado correctamente")
                
                # Cargar tokenizer y label encoder
                with open(tokenizer_path, 'rb') as f:
                    self.tokenizer = pickle.load(f)
                with open(label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                
                # Verificar que el modelo est√° correctamente cargado
                if self.model is None:
                    raise ValueError("El modelo no se carg√≥ correctamente")
                if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
                    raise ValueError("El tokenizer no se carg√≥ correctamente")
                if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                    raise ValueError("El label encoder no se carg√≥ correctamente")
                
                self.is_trained = True
                
                # Validaci√≥n final: asegurar que el modelo puede hacer una predicci√≥n de prueba
                print("üîç [DEBUG] Validando modelo con predicci√≥n de prueba...")
                try:
                    # Hacer una predicci√≥n de prueba para validar que el modelo funciona
                    test_text = "excelente"
                    print(f"üîç [DEBUG] Texto de prueba: '{test_text}'")
                    test_X = self.prepare_data([test_text])
                    print(f"üîç [DEBUG] Datos de prueba preparados: shape={test_X.shape}")
                    test_pred = self.model.predict(test_X, verbose=0)
                    print(f"üîç [DEBUG] Predicci√≥n de prueba: {test_pred}")
                    if test_pred is None or len(test_pred) == 0:
                        raise ValueError("El modelo no puede hacer predicciones v√°lidas")
                    print("‚úÖ [DEBUG] Modelo validado correctamente con predicci√≥n de prueba")
                except Exception as e:
                    print(f"‚ö†Ô∏è [DEBUG] Error al validar modelo: {e}")
                    import traceback
                    traceback.print_exc()
                    # Si falla la validaci√≥n, marcar como no entrenado
                    self.is_trained = False
                    raise ValueError(f"El modelo no est√° funcionando correctamente: {str(e)}")
                
                print("‚úÖ Modelo de red neuronal cargado y verificado correctamente")
                return
            except Exception as e:
                print(f"‚ö†Ô∏è Error al cargar modelo pre-entrenado: {e}")
                import traceback
                traceback.print_exc()
                print("üîÑ Se crear√° un nuevo modelo...")
                # Limpiar archivos corruptos si existen
                try:
                    if os.path.exists(model_path):
                        os.remove(model_path)
                    if os.path.exists(tokenizer_path):
                        os.remove(tokenizer_path)
                    if os.path.exists(label_encoder_path):
                        os.remove(label_encoder_path)
                except:
                    pass
        
        # Si no existe o fall√≥ cargar, crear y entrenar modelo
        print("üîÑ Creando y entrenando modelo de red neuronal (versi√≥n r√°pida, ~10-20 segundos)...")
        print("üîç [DEBUG] Iniciando _create_pretrained_model()...")
        try:
            self._create_pretrained_model()
            print("‚úÖ Modelo de red neuronal entrenado y guardado correctamente")
            
            # Validar que el modelo est√© completamente listo despu√©s del entrenamiento
            print("üîç [DEBUG] Validando modelo despu√©s del entrenamiento...")
            if not self.is_trained:
                raise ValueError("El modelo no se marc√≥ como entrenado despu√©s de _create_pretrained_model()")
            if not self.model:
                raise ValueError("El modelo no se cre√≥ despu√©s de _create_pretrained_model()")
            if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
                raise ValueError("El tokenizer no se entren√≥ correctamente")
            if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                raise ValueError("El label encoder no se entren√≥ correctamente")
            
            print("‚úÖ [DEBUG] Modelo completamente validado despu√©s del entrenamiento")
        except Exception as e:
            print(f"‚ùå Error al crear modelo de red neuronal: {e}")
            import traceback
            traceback.print_exc()
            self.is_trained = False
            raise
    
    def _create_pretrained_model(self):
        """Entrenar red neuronal LSTM con comentarios de hasta 25 palabras"""
        print("üîç [DEBUG] _create_pretrained_model() iniciado")
        # Datos de entrenamiento con comentarios completos (hasta 25 palabras)
        # Incluir frases cortas Y comentarios completos para mejor aprendizaje
        
        # Datos de entrenamiento MEJORADOS con muchas m√°s palabras clave y ejemplos
        # Incluir variaciones de palabras comunes en espa√±ol
        
        # Comentarios POSITIVOS (palabras clave: excelente, bueno, buena, genial, etc.)
        positive_texts = [
            # Palabras clave simples
            "excelente", "bueno", "buena", "genial", "perfecto", "perfecta",
            "incre√≠ble", "maravilloso", "fant√°stico", "s√∫per", "s√∫per bien",
            # Frases comunes positivas
            "excelente producto", "buen producto", "muy buen producto", "producto excelente",
            "excelente servicio", "buen servicio", "muy buen servicio", "servicio excelente",
            "excelente atenci√≥n", "buena atenci√≥n", "muy buena atenci√≥n", "atenci√≥n excelente",
            "excelente calidad", "buena calidad", "muy buena calidad", "calidad excelente",
            # Frases completas positivas
            "excelente producto muy bueno", "me encanta este servicio", "muy satisfecho",
            "recomiendo totalmente", "calidad superior", "atenci√≥n perfecta",
            "super contento", "vale la pena", "muy recomendado", "incre√≠ble experiencia",
            "producto genial", "muy bien hecho", "s√∫per recomendable",
            "excelente servicio al cliente", "servicio excelente",
            "muy buena experiencia", "experiencia excelente", "experiencia positiva",
            "altamente recomendado", "muy recomendable", "totalmente recomendado",
            "muy contento", "satisfecho completamente", "me gust√≥ mucho",
            "funciona perfecto", "cumple expectativas", "supera expectativas",
        ]
        
        # Comentarios NEGATIVOS (palabras clave: mal, malo, p√©simo, insultos, etc.)
        negative_texts = [
            # Palabras clave simples negativas
            "mal", "malo", "mala", "p√©simo", "p√©sima", "terrible", "horrible",
            "basura", "ruin", "decepcionante", "decepcionado",
            # Insultos y expresiones negativas comunes
            "esta cagada", "es una mierda", "una porquer√≠a", "es basura",
            "no sirve", "no funciona", "no vale", "no recomiendo",
            # Frases comunes negativas
            "p√©simo servicio", "mal servicio", "muy mal servicio", "servicio p√©simo",
            "p√©simo producto", "mal producto", "muy mal producto", "producto p√©simo",
            "p√©sima atenci√≥n", "mal atenci√≥n", "muy mal atenci√≥n", "atenci√≥n p√©sima",
            "p√©sima calidad", "mal calidad", "muy mal calidad", "calidad p√©sima",
            # Frases completas negativas
            "p√©simo servicio muy malo", "no recomiendo para nada", "calidad terrible",
            "muy decepcionado", "atenci√≥n horrible", "lento e ineficiente", "no vale la pena",
            "muy insatisfecho", "problema grave", "no cumpli√≥ expectativas", "servicio p√©simo",
            "muy mala calidad", "no funciona bien", "muy decepcionante",
            "muy mal", "horrible experiencia", "p√©sima experiencia", "experiencia negativa",
            "no lo recomiendo", "no vale nada", "totalmente insatisfecho",
            "funciona mal", "no cumple expectativas", "muy por debajo de lo esperado",
        ]
        
        # Comentarios NEUTRALES (palabras clave: normal, regular, aceptable, etc.)
        neutral_texts = [
            # Palabras clave simples neutrales
            "normal", "regular", "aceptable", "b√°sico", "est√°ndar", "com√∫n",
            "ni bueno ni malo", "ni mal ni bien", "sin m√°s", "nada especial",
            # Frases comunes neutrales
            "producto regular", "servicio regular", "atenci√≥n regular", "calidad regular",
            "producto normal", "servicio normal", "atenci√≥n normal", "calidad normal",
            "producto aceptable", "servicio aceptable", "atenci√≥n aceptable", "calidad aceptable",
            # Frases completas neutrales
            "ni bueno ni malo", "aceptable", "sin comentarios",
            "b√°sico", "est√°ndar", "cumple su funci√≥n", "nada especial", "producto com√∫n",
            "servicio est√°ndar", "normal como cualquier otro", "ni destacable ni malo",
            "producto promedio", "servicio b√°sico", "cumple con lo b√°sico",
            "ni destacable ni malo", "regular nada m√°s", "como se esperaba",
            "sin sorpresas", "ni bueno ni mal", "est√° bien",
        ]
        
        texts = positive_texts + negative_texts + neutral_texts
        labels = (['positivo'] * len(positive_texts) + 
                 ['negativo'] * len(negative_texts) + 
                 ['neutral'] * len(neutral_texts))
        
        print("üîÑ Entrenando red neuronal LSTM para comentarios de hasta 25 palabras...")
        print(f"üìä Total de textos: {len(texts)}, Clases: {len(set(labels))}")
        print(f"üîç [DEBUG] Textos positivos: {len(positive_texts)}, negativos: {len(negative_texts)}, neutrales: {len(neutral_texts)}")
        
        # Entrenamiento con m√°s √©pocas para mejor aprendizaje
        print("üîç [DEBUG] Iniciando entrenamiento...")
        try:
            # Entrenamiento r√°pido pero efectivo: 2 √©pocas con m√°s datos
            history = self.train(texts, labels, epochs=2, batch_size=32)  # 2 √©pocas, batch m√°s grande para velocidad
            print("‚úÖ [DEBUG] M√©todo train() completado")
            
            # Validar que el modelo est√° entrenado
            print(f"üîç [DEBUG] Verificando estado del modelo despu√©s del entrenamiento...")
            print(f"üîç [DEBUG] is_trained: {self.is_trained}")
            print(f"üîç [DEBUG] model existe: {self.model is not None}")
            print(f"üîç [DEBUG] tokenizer tiene word_index: {hasattr(self.tokenizer, 'word_index') and self.tokenizer.word_index is not None}")
            print(f"üîç [DEBUG] label_encoder tiene classes: {hasattr(self.label_encoder, 'classes_') and len(self.label_encoder.classes_) > 0}")
            
            if not self.is_trained:
                raise ValueError("El modelo no se marc√≥ como entrenado despu√©s del entrenamiento")
            if not self.model:
                raise ValueError("El modelo no existe despu√©s del entrenamiento")
            
            print("üîç [DEBUG] Guardando modelo...")
            self.save_model()
            print("‚úÖ [DEBUG] Modelo guardado correctamente")
            
            # Validaci√≥n final: hacer una predicci√≥n de prueba
            print("üîç [DEBUG] Haciendo predicci√≥n de prueba despu√©s del entrenamiento...")
            test_result = self.predict_single("excelente servicio")
            print(f"‚úÖ [DEBUG] Predicci√≥n de prueba exitosa: sentiment={test_result.get('sentiment')}, score={test_result.get('score')}")
            
        except Exception as e:
            print(f"‚ùå [DEBUG] Error en _create_pretrained_model: {str(e)}")
            import traceback
            traceback.print_exc()
            self.is_trained = False
            raise
        
        print("‚úÖ Red neuronal LSTM entrenada correctamente (soporta comentarios de hasta 25 palabras)")
    
    def save_model(self, model_path: str = 'app/ml_models/sentiment_model.h5'):
        """Guardar modelo"""
        model_dir = os.path.dirname(model_path)
        os.makedirs(model_dir, exist_ok=True)
        
        if self.model:
            self.model.save(model_path)
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(self.tokenizer, f)
        with open(label_encoder_path, 'wb') as f:
            pickle.dump(self.label_encoder, f)

