import re
import numpy as np
from typing import Dict, List, Tuple
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback
import time

class TrainingProgressCallback(Callback):
    """Callback para monitorear el progreso del entrenamiento"""
    def __init__(self):
        self.start_time = None
        self.epoch_times = []
        import sys
        self.stdout = sys.stdout
    
    def _print_and_flush(self, message):
        """Imprimir y hacer flush inmediatamente"""
        print(message)
        self.stdout.flush()
    
    def on_train_begin(self, logs=None):
        self.start_time = time.time()
        self._print_and_flush(f"‚è±Ô∏è [DEBUG] Entrenamiento comenz√≥ a las {time.strftime('%H:%M:%S')}")
        self._print_and_flush(f"üîç [DEBUG] Callback on_train_begin ejecutado correctamente")
    
    def on_epoch_begin(self, epoch, logs=None):
        epoch_start = time.time()
        self._print_and_flush(f"üîÑ [DEBUG] √âpoca {epoch + 1} comenzando a las {time.strftime('%H:%M:%S')}...")
        self.current_epoch_start = epoch_start
    
    def on_batch_begin(self, batch, logs=None):
        # Log cada 5 batches para no saturar, pero ver progreso
        if batch % 5 == 0 or batch == 0:
            self._print_and_flush(f"üîç [DEBUG] Batch {batch} comenzando...")
    
    def on_batch_end(self, batch, logs=None):
        # Log cada 5 batches para no saturar, pero ver progreso
        if batch % 5 == 0 or batch == 0:
            loss = logs.get('loss', 'N/A')
            acc = logs.get('accuracy', 'N/A')
            self._print_and_flush(f"üîç [DEBUG] Batch {batch} completado - loss: {loss}, accuracy: {acc}")
    
    def on_epoch_end(self, epoch, logs=None):
        epoch_time = time.time() - self.current_epoch_start
        self.epoch_times.append(epoch_time)
        loss = logs.get('loss', 'N/A')
        accuracy = logs.get('accuracy', 'N/A')
        val_loss = logs.get('val_loss', 'N/A')
        val_accuracy = logs.get('val_accuracy', 'N/A')
        self._print_and_flush(f"‚úÖ [DEBUG] √âpoca {epoch + 1} completada en {epoch_time:.2f}s - loss: {loss:.4f}, accuracy: {accuracy:.4f}, val_loss: {val_loss}, val_accuracy: {val_accuracy}")
    
    def on_train_end(self, logs=None):
        total_time = time.time() - self.start_time
        self._print_and_flush(f"‚è±Ô∏è [DEBUG] Entrenamiento terminado en {total_time:.2f}s total")
        if self.epoch_times:
            avg_time = sum(self.epoch_times) / len(self.epoch_times)
            self._print_and_flush(f"üìä [DEBUG] Tiempo promedio por √©poca: {avg_time:.2f}s")
        else:
            self._print_and_flush(f"‚ö†Ô∏è [DEBUG] No se registraron √©pocas completadas")

class SentimentNeuralNetwork:
    def __init__(self, max_words=5000, max_len=100):
        # Red neuronal LSTM basada en texto - Optimizado para p√°rrafos largos
        # max_words: 5000 (vocabulario amplio para mejor comprensi√≥n)
        # max_len: 100 (longitud suficiente para p√°rrafos completos)
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
        self.label_encoder = LabelEncoder()
        self.model = None
        self.is_trained = False
        
    def clean_text(self, text: str) -> str:
        """Limpieza de texto mejorada con normalizaci√≥n y correcci√≥n de encoding"""
        if not text:
            return ""
        
        # Primero, intentar corregir problemas de encoding comunes de Excel/CSV
        # Problemas comunes: √É¬© -> √©, √É¬≥ -> √≥, √É¬± -> √±, etc.
        # Esto ocurre cuando Excel guarda UTF-8 pero se lee como Latin-1
        encoding_fixes = {
            # Caracteres mal codificados m√°s comunes (UTF-8 mal le√≠do como Latin-1)
            '√É¬°': '√°', '√É¬©': '√©', '√É¬≠': '√≠', '√É¬≥': '√≥', '√É¬∫': '√∫',
            '√É¬±': '√±', '√É¬º': '√º', 
            '√É': '√Å', '√É‚Ä∞': '√â', '√É': '√ç', '√É"': '√ì', '√É≈°': '√ö',
            '√É': '√ë', '√É≈ì': '√ú',
            # Caracteres raros que aparecen en Excel
            '√¢‚Ç¨‚Ñ¢': "'", '√¢‚Ç¨≈ì': '"', '√¢‚Ç¨': '"', '√¢‚Ç¨"': '‚Äî', '√¢‚Ç¨"': '‚Äì',
            # Limpiar caracteres de control
            '\ufeff': '',  # BOM de UTF-8
            '\x00': '',  # Null bytes
        }
        
        # Aplicar correcciones de encoding
        for wrong, correct in encoding_fixes.items():
            text = text.replace(wrong, correct)
        
        # Intentar correcci√≥n autom√°tica de encoding si detectamos problemas
        if '√É' in text:
            try:
                # Intentar decodificar como Latin-1 y recodificar como UTF-8
                text = text.encode('latin-1', errors='ignore').decode('utf-8', errors='ignore')
            except:
                pass
        
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Normalizar tildes y caracteres especiales
        # IMPORTANTE: El modelo fue entrenado eliminando tildes para normalizar
        # "atenci√≥n" y "atencion" se tratan igual, lo cual es √∫til para el modelo
        replacements = {
            '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
            '√±': 'n', '√º': 'u'
        }
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # Eliminar URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Eliminar menciones y hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        
        # Eliminar caracteres especiales excepto letras, n√∫meros y espacios
        text = re.sub(r'[^a-z0-9\s]', ' ', text)
        
        # Eliminar espacios m√∫ltiples
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def prepare_data(self, texts: List[str], labels: List[str] = None) -> Tuple:
        """Preparar datos para entrenamiento o predicci√≥n"""
        # Logging m√≠nimo para ahorrar memoria durante predicci√≥n
        if labels:
            print(f"üîç [DEBUG] prepare_data() entrenamiento: {len(texts)} textos")
        # Si no hay labels, es predicci√≥n - logging m√≠nimo
        
        if not texts:
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        # Limpiar textos
        cleaned_texts = [self.clean_text(text) if text else "" for text in texts]
        
        # Tokenizar
        if labels:
            # Si hay etiquetas, estamos entrenando, ajustar tokenizer
            self.tokenizer.fit_on_texts(cleaned_texts)
            if len(self.tokenizer.word_index) > 0:
                print(f"üîç [DEBUG] Tokenizer entrenado: vocab_size={len(self.tokenizer.word_index)}")
        elif not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            raise ValueError("El tokenizer no est√° entrenado. Debe entrenar el modelo primero.")
        
        # Convertir textos a secuencias
        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)
        
        # Asegurar que todas las secuencias tengan al menos un elemento (OOV token)
        sequences = [seq if seq else [1] for seq in sequences]
        
        # Hacer padding
        padded_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post', truncating='post')
        
        if labels:
            encoded_labels = self.label_encoder.fit_transform(labels)
            # Mostrar distribuci√≥n de etiquetas (logging m√≠nimo)
            unique_encoded, counts_encoded = np.unique(encoded_labels, return_counts=True)
            label_names_encoded = self.label_encoder.inverse_transform(unique_encoded)
            print(f"üîç [DEBUG] Datos preparados: shape={padded_sequences.shape}, Distribuci√≥n: {dict(zip(label_names_encoded, counts_encoded))}")
            return padded_sequences, encoded_labels
        
        return padded_sequences
    
    def build_model(self, vocab_size: int, num_classes: int):
        """Construir red neuronal LSTM basada en texto para comentarios de hasta 25 palabras"""
        print(f"üîç [DEBUG] Construyendo modelo: vocab_size={vocab_size}, num_classes={num_classes}")
        print(f"üîç [DEBUG] Par√°metros del modelo: max_words={self.max_words}, max_len={self.max_len}")
        
        # Red neuronal LSTM con suficiente capacidad para aprender
        from tensorflow.keras.initializers import GlorotUniform
        
        # Modelo optimizado para mejor aprendizaje (aumentado de tama√±o m√≠nimo)
        # Balance entre memoria y capacidad de aprendizaje
        effective_vocab_size = min(vocab_size + 1, self.max_words + 1)
        model = Sequential([
            Embedding(effective_vocab_size, 16, mask_zero=True),  # 16 dimensiones (aumentado de 6)
            LSTM(8, dropout=0.2, recurrent_dropout=0.2),  # 8 unidades (aumentado de 3) con dropout
            Dense(16, activation='relu'),   # 16 unidades (aumentado de 6)
            Dropout(0.3),  # Dropout para regularizaci√≥n
            Dense(num_classes, activation='softmax')  # Salida (3 clases)
        ])
        print(f"üîç [DEBUG] Vocabulario: {effective_vocab_size}, Modelo mejorado: Embedding(16), LSTM(8), Dense(16)")
        
        print(f"üîç [DEBUG] Modelo construido, compilando...")
        # Compilar modelo neuronal con learning rate m√°s conservador para mejor convergencia
        from tensorflow.keras.optimizers import Adam
        optimizer = Adam(learning_rate=0.001)  # Learning rate m√°s conservador (0.001) para mejor convergencia
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy'],
            run_eagerly=False  # Deshabilitar eager mode para mejor rendimiento y convergencia
        )
        
        # NO contar par√°metros aqu√≠ - el modelo a√∫n no est√° "built"
        # Los par√°metros se contar√°n despu√©s del primer fit() cuando el modelo se construya autom√°ticamente
        print(f"üîç [DEBUG] Modelo compilado correctamente (run_eagerly=False)")
        
        return model
    
    def train(self, texts: List[str], labels: List[str], epochs=10, batch_size=32):
        """Entrenar modelo - Versi√≥n ULTRA-LIGERA para Render (512 MB limit)"""
        import tensorflow as tf
        import gc  # Para limpiar memoria
        
        print(f"üìä Preparando datos: {len(texts)} textos, {len(set(labels))} clases")
        X, y = self.prepare_data(texts, labels)
        
        # Mostrar distribuci√≥n de etiquetas ANTES de reducir
        unique_labels, counts = np.unique(y, return_counts=True)
        label_names = self.label_encoder.inverse_transform(unique_labels)
        print(f"üîç [DEBUG] Distribuci√≥n de etiquetas ANTES de reducir:")
        for label_name, count in zip(label_names, counts):
            print(f"   - {label_name}: {count} muestras")
        
        # USAR m√°s datos para mejor aprendizaje (pero sin exceder memoria)
        # Aumentar muestras para mejor precisi√≥n con p√°rrafos largos
        max_samples = 180  # Usar 180 muestras para mejor aprendizaje (aumentado de 120 para incluir p√°rrafos largos)
        if len(X) > max_samples:
            print(f"‚ö†Ô∏è Reduciendo datos de {len(X)} a {max_samples} para ahorrar memoria...")
            
            # MEZCLAR datos ANTES de reducir para mantener balance de clases
            indices = np.arange(len(X))
            np.random.seed(42)  # Semilla fija para reproducibilidad
            np.random.shuffle(indices)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            # Asegurar balance de clases al reducir
            unique_labels_all = np.unique(y_shuffled)
            num_classes_available = len(unique_labels_all)
            samples_per_class = max_samples // num_classes_available
            min_samples_per_class = max(1, samples_per_class - 2)  # Al menos samples_per_class-2 por clase
            
            print(f"üîç [DEBUG] Intentando balancear: ~{samples_per_class} muestras por clase de {num_classes_available} clases")
            
            # Recopilar muestras balanceadas
            X_balanced = []
            y_balanced = []
            samples_taken_per_class = {int(label): 0 for label in unique_labels_all}
            used_indices = set()
            
            # Primero, tomar muestras balanceadas de cada clase
            for label in unique_labels_all:
                label_int = int(label)
                label_indices = np.where(y_shuffled == label)[0]
                np.random.shuffle(label_indices)
                
                for idx in label_indices[:min_samples_per_class]:
                    if len(X_balanced) >= max_samples:
                        break
                    if idx not in used_indices:
                        X_balanced.append(X_shuffled[idx])
                        y_balanced.append(y_shuffled[idx])
                        used_indices.add(idx)
                        samples_taken_per_class[label_int] += 1
                
                if len(X_balanced) >= max_samples:
                    break
            
            # Si a√∫n hay espacio, tomar muestras adicionales de manera aleatoria
            remaining_indices = [i for i in range(len(X_shuffled)) if i not in used_indices]
            np.random.shuffle(remaining_indices)
            
            for idx in remaining_indices:
                if len(X_balanced) >= max_samples:
                    break
                X_balanced.append(X_shuffled[idx])
                y_balanced.append(y_shuffled[idx])
                used_indices.add(idx)
            
            X = np.array(X_balanced)
            y = np.array(y_balanced)
            
            # Validar balance de clases DESPU√âS de reducir
            unique_labels_reduced, counts_reduced = np.unique(y, return_counts=True)
            label_names_reduced = self.label_encoder.inverse_transform(unique_labels_reduced)
            print(f"üîç [DEBUG] Distribuci√≥n de etiquetas DESPU√âS de reducir (balanceado):")
            for label_name, count in zip(label_names_reduced, counts_reduced):
                print(f"   - {label_name}: {count} muestras")
        else:
            # MEZCLAR datos antes de entrenar para mejor aprendizaje
            print("üîç [DEBUG] Mezclando datos antes de entrenar...")
            indices = np.arange(len(X))
            np.random.seed(42)  # Semilla fija para reproducibilidad
            np.random.shuffle(indices)
            X = X[indices]
            y = y[indices]
            print(f"‚úÖ [DEBUG] Datos mezclados: {len(X)} muestras")
        
        # SIEMPRE entrenar sin validaci√≥n para m√°xima velocidad y menor uso de memoria
        # Con pocos datos, la validaci√≥n no es necesaria y solo ralentiza
        X_train, y_train = X, y
        X_val, y_val = X, y
        use_validation = False
        print(f"üîç [DEBUG] Entrenando SIN validaci√≥n para m√°xima velocidad y menor memoria")
        
        vocab_size = len(self.tokenizer.word_index)
        num_classes = len(self.label_encoder.classes_)
        print(f"üìä Vocabulario: {vocab_size} palabras, Clases: {num_classes}")
        print(f"üìä Datos entrenamiento: {len(X_train)}, Validaci√≥n: {len(X_val) if use_validation else 'N/A'}")
        
        # Validar balance de clases en datos de entrenamiento
        unique_labels_train, counts_train = np.unique(y_train, return_counts=True)
        label_names_train = self.label_encoder.inverse_transform(unique_labels_train)
        print(f"üîç [DEBUG] Distribuci√≥n de etiquetas en datos de ENTRENAMIENTO:")
        for label_name, count in zip(label_names_train, counts_train):
            print(f"   - {label_name}: {count} muestras")
        
        # Verificar que haya al menos una muestra de cada clase en entrenamiento
        if len(unique_labels_train) < num_classes:
            print(f"‚ö†Ô∏è [DEBUG] ADVERTENCIA: Solo hay {len(unique_labels_train)} clases en entrenamiento, esperadas {num_classes}")
            print(f"‚ö†Ô∏è [DEBUG] Esto puede afectar la precisi√≥n del modelo")
        else:
            print(f"‚úÖ [DEBUG] Todas las clases ({num_classes}) est√°n representadas en los datos de entrenamiento")
        
        # Limpiar memoria ANTES de construir modelo (CR√çTICO para Render 512 MB)
        print("üîç [DEBUG] Limpiando memoria antes de construir modelo...")
        import tensorflow as tf
        tf.keras.backend.clear_session()  # Limpiar sesi√≥n de Keras antes
        gc.collect()  # Recolectar basura
        print("‚úÖ [DEBUG] Memoria limpiada antes de construir modelo")
        
        print("üîç [DEBUG] Construyendo modelo...")
        build_start = time.time()
        self.model = self.build_model(vocab_size, num_classes)
        build_time = time.time() - build_start
        print(f"‚úÖ [DEBUG] Modelo construido en {build_time:.2f}s")
        
        # OPTIMIZACI√ìN: Balance entre memoria y aprendizaje
        actual_epochs = 15  # Aumentar √©pocas para mejor aprendizaje (aumentado de 5)
        # Batch size balanceado para mejor aprendizaje
        actual_batch_size = min(8, len(X_train))  # Batch size aumentado para mejor estabilidad (aumentado de 3)
        print(f"üîç [DEBUG] Batch size: {actual_batch_size}, √âpocas: {actual_epochs} (optimizado para mejor aprendizaje)")
        
        print(f"üöÄ Iniciando entrenamiento: {actual_epochs} √©pocas (reducido de {epochs}), batch_size={actual_batch_size} (ajustado de {batch_size})")
        print(f"üìä Datos de entrenamiento: {len(X_train)} muestras")
        print(f"üìä Shape de X_train: {X_train.shape}, Shape de y_train: {y_train.shape}")
        
        # Callbacks simples para entrenamiento
        fit_kwargs = {
            'epochs': actual_epochs,
            'batch_size': actual_batch_size,
            'verbose': 1,  # Mostrar progress (se cambia a 1 en fit())
            'callbacks': []  # Sin callbacks complejos para velocidad
        }
        
        # NO construir modelo expl√≠citamente - ahorra memoria
        # El modelo se construir√° autom√°ticamente en el primer fit()
        print("üîç [DEBUG] El modelo se construir√° autom√°ticamente en el primer fit()")
        
        # Entrenamiento SIMPLIFICADO - sin validaci√≥n, sin callbacks, m√°ximo velocidad
        print("üîç [DEBUG] Llamando a model.fit() sin validaci√≥n...")
        print(f"üîç [DEBUG] X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
        print(f"üîç [DEBUG] Par√°metros: epochs={actual_epochs}, batch_size={actual_batch_size}, samples={len(X_train)}")
        
        # Flush stdout para asegurar que los logs se muestren
        import sys
        sys.stdout.flush()
        
        print(f"üöÄ [DEBUG] INICIANDO model.fit() - entrenamiento con {actual_epochs} √©pocas...")
        sys.stdout.flush()
        
        fit_start = time.time()
        history = None
        try:
            # Entrenamiento con verbose para ver accuracy
            fit_kwargs['verbose'] = 1  # Mostrar progress para ver si est√° aprendiendo
            history = self.model.fit(X_train, y_train, **fit_kwargs)
            fit_time = time.time() - fit_start
            print(f"‚úÖ [DEBUG] model.fit() completado en {fit_time:.2f}s")
            
            # Verificar accuracy final
            if hasattr(history, 'history') and 'accuracy' in history.history:
                final_accuracy = history.history['accuracy'][-1]
                final_loss = history.history['loss'][-1] if 'loss' in history.history else None
                print(f"üìä [DEBUG] Accuracy final del entrenamiento: {final_accuracy:.4f}")
                if final_loss:
                    print(f"üìä [DEBUG] Loss final del entrenamiento: {final_loss:.4f}")
                if final_accuracy < 0.6:
                    print(f"‚ö†Ô∏è [DEBUG] ADVERTENCIA: Accuracy baja ({final_accuracy:.4f}), el modelo podr√≠a no estar aprendiendo bien")
                elif final_accuracy < 0.8:
                    print(f"‚úÖ [DEBUG] Accuracy aceptable: {final_accuracy:.4f} (mejorable pero funcional)")
                else:
                    print(f"‚úÖ [DEBUG] Accuracy excelente: {final_accuracy:.4f}")
            else:
                print(f"‚ö†Ô∏è [DEBUG] No se pudo obtener accuracy del historial")
            
            sys.stdout.flush()
        except Exception as fit_error:
            fit_time = time.time() - fit_start
            print(f"‚ùå [DEBUG] ERROR en model.fit() despu√©s de {fit_time:.2f}s: {str(fit_error)}")
            print(f"üîç [DEBUG] Tipo de error: {type(fit_error).__name__}")
            import traceback
            traceback.print_exc()
            sys.stdout.flush()
            raise
        
        # Ahora s√≠ podemos contar los par√°metros (el modelo ya est√° "built" despu√©s del fit)
        try:
            total_params = self.model.count_params()
            print(f"üìä [DEBUG] Modelo entrenado - Total de par√°metros: {total_params:,}")
        except Exception as e:
            print(f"‚ö†Ô∏è [DEBUG] No se pudo contar par√°metros: {e}")
        
        print(f"‚úÖ Entrenamiento completado (sin validaci√≥n)")
        
        # Validar que el modelo est√© correctamente entrenado
        print("üîç [DEBUG] Validando modelo despu√©s del entrenamiento...")
        if self.model is None:
            raise ValueError("El modelo no existe despu√©s del entrenamiento")
        
        print("üîç [DEBUG] Marcando modelo como entrenado...")
        self.is_trained = True
        print(f"‚úÖ [DEBUG] Modelo marcado como entrenado: is_trained={self.is_trained}")
        print(f"üîç [DEBUG] Modelo existe: {self.model is not None}")
        print(f"üîç [DEBUG] Tokenizer tiene word_index: {hasattr(self.tokenizer, 'word_index') and len(self.tokenizer.word_index) > 0}")
        print(f"üîç [DEBUG] Label encoder tiene classes: {hasattr(self.label_encoder, 'classes_') and len(self.label_encoder.classes_) > 0}")
        
        # NO hacer prueba r√°pida para ahorrar memoria
        # El modelo ya est√° entrenado y validado por el accuracy del entrenamiento
        print("üîç [DEBUG] Prueba r√°pida omitida para ahorrar memoria")
        
        # Limpiar memoria despu√©s de validar (CR√çTICO para Render 512 MB)
        print("üîç [DEBUG] Limpiando memoria despu√©s de entrenar...")
        # NO eliminar history aqu√≠ porque se necesita devolver
        # Solo limpiar otras variables temporales
        gc.collect()  # Recolectar basura de Python
        print("‚úÖ [DEBUG] Memoria limpiada (modelo preservado)")
        
        # Devolver history si existe, sino devolver None
        return history if history is not None else None
    
    def predict(self, texts: List[str]) -> List[Dict]:
        """Predecir sentimiento usando red neuronal LSTM"""
        print(f"üîç [DEBUG] predict() llamado con {len(texts)} texto(s)")
        
        # Validar que el modelo est√© completamente entrenado y listo
        print(f"üîç [DEBUG] Validando modelo: is_trained={self.is_trained}, model={self.model is not None}")
        if not self.is_trained:
            print("‚ùå [DEBUG] Error: modelo no est√° entrenado")
            raise ValueError(
                "El modelo de red neuronal no est√° entrenado. "
                "El modelo se est√° cargando o entrenando. Por favor, espera unos momentos."
            )
        
        if not self.model:
            print("‚ùå [DEBUG] Error: modelo no est√° inicializado")
            raise ValueError(
                "El modelo de red neuronal no est√° inicializado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        # Validar que el tokenizer est√© entrenado
        print(f"üîç [DEBUG] Validando tokenizer: has word_index={hasattr(self.tokenizer, 'word_index')}")
        if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
            print("‚ùå [DEBUG] Error: tokenizer no tiene word_index")
            raise ValueError(
                "El tokenizer del modelo no est√° entrenado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        # Validar que el label encoder est√© entrenado
        print(f"üîç [DEBUG] Validando label_encoder: has classes={hasattr(self.label_encoder, 'classes_')}")
        if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
            print("‚ùå [DEBUG] Error: label_encoder no tiene classes")
            raise ValueError(
                "El label encoder del modelo no est√° entrenado. "
                "El modelo se est√° cargando. Por favor, espera unos momentos."
            )
        
        if not texts:
            print("‚ùå [DEBUG] Error: lista de textos vac√≠a")
            raise ValueError("La lista de textos no puede estar vac√≠a")
        
        try:
            # Preparar datos para predicci√≥n (logging m√≠nimo para ahorrar memoria)
            X = self.prepare_data(texts)
            # Limpiar memoria inmediatamente despu√©s de preparar datos
            import gc
            gc.collect()
            
            # Verificar que tenemos datos v√°lidos
            if X.shape[0] == 0:
                print("‚ùå [DEBUG] Error: X.shape[0] == 0")
                raise ValueError("No se pudieron preparar los datos para predicci√≥n")
            
            print("üîç [DEBUG] Haciendo predicci√≥n con modelo neuronal...")
            # Hacer predicci√≥n con batch_size=1 para m√≠nimo uso de memoria
            # Procesar uno por uno para evitar problemas de memoria
            predictions = self.model.predict(X, batch_size=1, verbose=0)
            print(f"üîç [DEBUG] Predicciones recibidas: shape={predictions.shape if predictions is not None else None}")
            
            # Validar predicciones
            if predictions is None:
                print("‚ùå [DEBUG] Error: predictions es None")
                raise ValueError("El modelo no devolvi√≥ predicciones (None)")
            
            if len(predictions) == 0:
                print("‚ùå [DEBUG] Error: predictions est√° vac√≠o")
                raise ValueError("El modelo no devolvi√≥ predicciones (vac√≠o)")
            
            print(f"üîç [DEBUG] Procesando predicciones...")
            # Mostrar solo la primera predicci√≥n para ahorrar memoria (logging m√≠nimo)
            label_names = self.label_encoder.classes_
            if len(predictions) > 0:
                probs = predictions[0]
                prob_dict = dict(zip(label_names, probs))
                print(f"üîç [DEBUG] Predicci√≥n 0: {prob_dict}")
            
            # Procesar predicciones de la red neuronal (m√≠nimo logging para ahorrar memoria)
            predicted_classes = np.argmax(predictions, axis=1)
            predicted_labels = self.label_encoder.inverse_transform(predicted_classes)
            confidence = np.max(predictions, axis=1)
            
            # Formatear confianza correctamente
            if len(confidence) > 0 and len(predicted_labels) > 0:
                conf_value = confidence[0]
                label_value = predicted_labels[0]
                print(f"üîç [DEBUG] Predicci√≥n: {label_value}, confianza: {conf_value:.3f}")
            else:
                print(f"üîç [DEBUG] Predicci√≥n: N/A, confianza: 0.000")
            
            results = []
            for i, text in enumerate(texts):
                if i >= len(predicted_labels):
                    print(f"‚ö†Ô∏è [DEBUG] Advertencia: √≠ndice {i} fuera de rango para predicciones")
                    label = 'neutral'
                    score = 0.5
                else:
                    label = predicted_labels[i]
                    score = float(confidence[i])
                
                if label == 'positivo':
                    sentiment = 'positivo'
                    emoji = 'üü¢'
                    score_value = score
                elif label == 'negativo':
                    sentiment = 'negativo'
                    emoji = 'üî¥'
                    score_value = -score
                else:
                    sentiment = 'neutral'
                    emoji = 'üü°'
                    score_value = 0.0
                
                results.append({
                    'text': text,
                    'sentiment': sentiment,
                    'score': round(score_value, 3),
                    'emoji': emoji,
                    'confidence': round(score, 3)
                })
            
            print(f"‚úÖ [DEBUG] Predicci√≥n completada: {len(results)} resultado(s)")
            return results
            
        except ValueError as e:
            # Re-lanzar ValueError con mensaje claro
            error_msg = str(e)
            print(f"‚ùå [DEBUG] ValueError en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"Error en predicci√≥n de red neuronal: {str(e)}"
            print(f"‚ùå [DEBUG] Exception en predict: {error_msg}")
            import traceback
            traceback.print_exc()
            raise ValueError(error_msg)
    
    def predict_single(self, text: str) -> Dict:
        """Predecir sentimiento de un solo texto"""
        print(f"üîç [DEBUG] predict_single() llamado con texto: '{text[:50]}...'")
        try:
            results = self.predict([text])
            if not results or len(results) == 0:
                print("‚ùå [DEBUG] Error: predict() no devolvi√≥ resultados")
                raise ValueError("No se obtuvieron resultados de la predicci√≥n")
            result = results[0]
            print(f"üîç [DEBUG] predict_single() resultado: sentiment={result.get('sentiment')}, score={result.get('score')}")
            return result
        except Exception as e:
            print(f"‚ùå [DEBUG] Error en predict_single: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def load_model(self, model_path: str = 'app/ml_models/sentiment_model.keras'):
        """Cargar modelo pre-entrenado - Descarga autom√°tica desde GitHub Releases si no existe"""
        # Asegurar que el directorio existe
        model_dir = os.path.dirname(model_path)
        if not os.path.exists(model_dir):
            os.makedirs(model_dir, exist_ok=True)
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        # URLs para descargar modelo pre-entrenado desde GitHub Releases
        # ACTUALIZA ESTAS URLs con las URLs reales despu√©s de subir a GitHub Releases
        MODEL_URL = os.getenv(
            'MODEL_URL', 
            'https://github.com/crisncr/inteligenciaArtificial/releases/download/v1.0.0/sentiment_model.keras'
        )
        TOKENIZER_URL = os.getenv(
            'TOKENIZER_URL',
            'https://github.com/crisncr/inteligenciaArtificial/releases/download/v1.0.0/tokenizer.pkl'
        )
        LABEL_ENCODER_URL = os.getenv(
            'LABEL_ENCODER_URL',
            'https://github.com/crisncr/inteligenciaArtificial/releases/download/v1.0.0/label_encoder.pkl'
        )
        
        def download_file(url: str, filepath: str) -> bool:
            """Descargar archivo desde URL"""
            try:
                import requests
                print(f"üì• Descargando {os.path.basename(filepath)} desde GitHub Releases...")
                response = requests.get(url, timeout=60, stream=True)
                response.raise_for_status()
                
                os.makedirs(os.path.dirname(filepath), exist_ok=True)
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                
                print(f"‚úÖ {os.path.basename(filepath)} descargado correctamente ({downloaded / 1024:.1f} KB)")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è No se pudo descargar {os.path.basename(filepath)}: {str(e)}")
                try:
                    if os.path.exists(filepath):
                        os.remove(filepath)
                except:
                    pass
                return False
        
        # Verificar qu√© archivos faltan
        missing_files = []
        if not os.path.exists(model_path):
            missing_files.append(('Modelo', MODEL_URL, model_path))
        if not os.path.exists(tokenizer_path):
            missing_files.append(('Tokenizer', TOKENIZER_URL, tokenizer_path))
        if not os.path.exists(label_encoder_path):
            missing_files.append(('Label Encoder', LABEL_ENCODER_URL, label_encoder_path))
        
        # Descargar archivos faltantes
        if missing_files:
            print(f"üì• Descargando {len(missing_files)} archivo(s) del modelo pre-entrenado desde GitHub Releases...")
            downloaded_count = 0
            for name, url, filepath in missing_files:
                print(f"üì• Descargando {name}...")
                if download_file(url, filepath):
                    downloaded_count += 1
                else:
                    print(f"‚ùå Error al descargar {name}")
            
            if downloaded_count < len(missing_files):
                print(f"‚ö†Ô∏è Solo se descargaron {downloaded_count}/{len(missing_files)} archivos.")
                print("üîÑ Se entrenar√° el modelo desde cero (esto tomar√° m√°s tiempo)...")
                print("üí° NOTA: Esto solo deber√≠a pasar si las URLs de GitHub Releases no est√°n disponibles")
                # Limpiar archivos parcialmente descargados
                for name, url, filepath in missing_files:
                    if os.path.exists(filepath):
                        try:
                            os.remove(filepath)
                        except:
                            pass
            else:
                print("‚úÖ Todos los archivos del modelo se descargaron correctamente desde GitHub Releases")
                print("‚úÖ El modelo NO se entrenar√°, se usar√° el modelo pre-entrenado")
        
        # Intentar cargar modelo existente (local o descargado)
        if os.path.exists(model_path) and os.path.exists(tokenizer_path) and os.path.exists(label_encoder_path):
            try:
                print("üîÑ Cargando modelo de red neuronal pre-entrenado...")
                # Cargar modelo en formato .keras (compatible con Keras 3.x)
                try:
                    # Intentar cargar directamente (formato .keras es m√°s compatible)
                    self.model = load_model(model_path)
                    print("‚úÖ Modelo cargado correctamente")
                except Exception as load_error:
                    print(f"‚ö†Ô∏è Error al cargar modelo: {load_error}")
                    # Si falla, intentar cargar sin compilaci√≥n
                    try:
                        self.model = load_model(model_path, compile=False)
                        # Recompilar el modelo
                        from tensorflow.keras.optimizers import Adam
                        self.model.compile(
                            optimizer=Adam(learning_rate=0.001),
                            loss='sparse_categorical_crossentropy',
                            metrics=['accuracy']
                        )
                        print("‚úÖ Modelo cargado y recompilado correctamente")
                    except Exception as compile_error:
                        print(f"‚ùå Error al recompilar modelo: {compile_error}")
                        raise
                
                # Cargar tokenizer y label encoder
                with open(tokenizer_path, 'rb') as f:
                    self.tokenizer = pickle.load(f)
                with open(label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                
                # Verificar que el modelo est√° correctamente cargado
                if self.model is None:
                    raise ValueError("El modelo no se carg√≥ correctamente")
                if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
                    raise ValueError("El tokenizer no se carg√≥ correctamente")
                if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                    raise ValueError("El label encoder no se carg√≥ correctamente")
                
                self.is_trained = True
                
                # Validaci√≥n final: asegurar que el modelo puede hacer una predicci√≥n de prueba
                print("üîç [DEBUG] Validando modelo con predicci√≥n de prueba...")
                try:
                    # Hacer una predicci√≥n de prueba para validar que el modelo funciona
                    test_text = "excelente"
                    print(f"üîç [DEBUG] Texto de prueba: '{test_text}'")
                    test_X = self.prepare_data([test_text])
                    print(f"üîç [DEBUG] Datos de prueba preparados: shape={test_X.shape}")
                    test_pred = self.model.predict(test_X, batch_size=1, verbose=0)
                    print(f"üîç [DEBUG] Predicci√≥n de prueba: {test_pred}")
                    if test_pred is None or len(test_pred) == 0:
                        raise ValueError("El modelo no puede hacer predicciones v√°lidas")
                    print("‚úÖ [DEBUG] Modelo validado correctamente con predicci√≥n de prueba")
                except Exception as e:
                    print(f"‚ö†Ô∏è [DEBUG] Error al validar modelo: {e}")
                    import traceback
                    traceback.print_exc()
                    # Si falla la validaci√≥n, marcar como no entrenado
                    self.is_trained = False
                    raise ValueError(f"El modelo no est√° funcionando correctamente: {str(e)}")
                
                print("‚úÖ Modelo de red neuronal cargado y verificado correctamente")
                return
            except Exception as e:
                print(f"‚ö†Ô∏è Error al cargar modelo pre-entrenado: {e}")
                import traceback
                traceback.print_exc()
                print("üîÑ Se crear√° un nuevo modelo...")
                # Limpiar archivos corruptos si existen
                try:
                    if os.path.exists(model_path):
                        os.remove(model_path)
                    if os.path.exists(tokenizer_path):
                        os.remove(tokenizer_path)
                    if os.path.exists(label_encoder_path):
                        os.remove(label_encoder_path)
                except:
                    pass
        
        # Si no existe o fall√≥ cargar, crear y entrenar modelo (FALLBACK)
        print("=" * 60)
        print("‚ö†Ô∏è MODO FALLBACK: ENTRENANDO MODELO DESDE CERO")
        print("=" * 60)
        print("üîÑ Creando y entrenando modelo de red neuronal (versi√≥n r√°pida, ~30-60 segundos)...")
        print("‚ö†Ô∏è NOTA: Esto se ejecuta solo si no se pudo descargar el modelo pre-entrenado")
        print("üí° RECOMENDACI√ìN: Sube los archivos a GitHub Releases para evitar este entrenamiento")
        print("üìã Ver train_model_local.py para instrucciones")
        print("üîç [DEBUG] Iniciando _create_pretrained_model()...")
        try:
            self._create_pretrained_model()
            print("‚úÖ Modelo de red neuronal entrenado y guardado correctamente")
            
            # Validar que el modelo est√© completamente listo despu√©s del entrenamiento
            print("üîç [DEBUG] Validando modelo despu√©s del entrenamiento...")
            if not self.is_trained:
                raise ValueError("El modelo no se marc√≥ como entrenado despu√©s de _create_pretrained_model()")
            if not self.model:
                raise ValueError("El modelo no se cre√≥ despu√©s de _create_pretrained_model()")
            if not hasattr(self.tokenizer, 'word_index') or not self.tokenizer.word_index:
                raise ValueError("El tokenizer no se entren√≥ correctamente")
            if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                raise ValueError("El label encoder no se entren√≥ correctamente")
            
            print("‚úÖ [DEBUG] Modelo completamente validado despu√©s del entrenamiento")
        except Exception as e:
            print(f"‚ùå Error al crear modelo de red neuronal: {e}")
            import traceback
            traceback.print_exc()
            self.is_trained = False
            raise
    
    def _create_pretrained_model(self):
        """Entrenar red neuronal LSTM con comentarios y p√°rrafos largos (hasta 100 palabras)"""
        print("üîç [DEBUG] _create_pretrained_model() iniciado")
        print("üìä Modelo configurado para p√°rrafos largos: max_len=100, max_words=5000")
        # Datos de entrenamiento con frases cortas Y p√°rrafos largos para mejor aprendizaje
        # Incluir variaciones de palabras comunes en espa√±ol y p√°rrafos completos
        # El modelo ahora puede procesar p√°rrafos completos de hasta 100 palabras
        
        # Comentarios POSITIVOS (palabras clave: excelente, bueno, buena, genial, etc.)
        positive_texts = [
            # Palabras clave simples
            "excelente", "bueno", "buena", "genial", "perfecto", "perfecta",
            "incre√≠ble", "maravilloso", "fant√°stico", "s√∫per", "s√∫per bien",
            # Frases comunes positivas
            "excelente producto", "buen producto", "muy buen producto", "producto excelente",
            "excelente servicio", "buen servicio", "muy buen servicio", "servicio excelente",
            "excelente atenci√≥n", "buena atenci√≥n", "muy buena atenci√≥n", "atenci√≥n excelente",
            "excelente calidad", "buena calidad", "muy buena calidad", "calidad excelente",
            # Frases completas positivas
            "excelente producto muy bueno", "me encanta este servicio", "muy satisfecho",
            "recomiendo totalmente", "calidad superior", "atenci√≥n perfecta",
            "super contento", "vale la pena", "muy recomendado", "incre√≠ble experiencia",
            "producto genial", "muy bien hecho", "s√∫per recomendable",
            "excelente servicio al cliente", "servicio excelente",
            "muy buena experiencia", "experiencia excelente", "experiencia positiva",
            "altamente recomendado", "muy recomendable", "totalmente recomendado",
            "muy contento", "satisfecho completamente", "me gust√≥ mucho",
            "funciona perfecto", "cumple expectativas", "supera expectativas",
            # Frases positivas adicionales (palabras que el modelo necesita aprender)
            "funciona de maravilla", "muy f√°cil de usar", "muy r√°pida",
            "fue amable", "resolvi√≥ mi problema", "soporte t√©cnico amable",
            "aplicaci√≥n funciona bien", "muy f√°cil", "r√°pida y eficiente",
            "lleg√≥ antes de lo esperado", "totalmente recomendado", "muy recomendable",
            "resolvi√≥ enseguida", "problema resuelto", "soporte excelente",
            "aplicaci√≥n f√°cil", "funciona bien", "muy r√°pido",
            "amable y servicial", "resolvi√≥ r√°pido", "soporte r√°pido",
            "f√°cil de usar", "muy eficiente", "funciona perfectamente",
            "de maravilla", "muy bien", "excelente atenci√≥n",
            "resolvi√≥ mi problema enseguida", "soporte t√©cnico excelente",
            "aplicaci√≥n funciona de maravilla", "muy f√°cil de usar y r√°pida",
            # P√°rrafos largos positivos
            "estoy muy satisfecho con este producto la calidad es excelente y el servicio al cliente fue incre√≠ble me respondieron r√°pido a todas mis preguntas y el producto lleg√≥ en perfectas condiciones sin duda lo recomiendo a todos",
            "me encanta este servicio la atenci√≥n que recib√≠ fue maravillosa desde el primer momento me sent√≠ bien atendido el producto funciona perfectamente y cumple con todas mis expectativas estoy muy contento con la compra",
            "excelente experiencia de compra el producto es de muy buena calidad y el servicio al cliente es excepcional me ayudaron con todas mis dudas y el env√≠o fue muy r√°pido sin duda volver√© a comprar aqu√≠",
            "estoy muy contento con este producto la calidad es superior a lo que esperaba y el servicio al cliente fue incre√≠ble me ayudaron con todas mis preguntas y el producto lleg√≥ en perfectas condiciones",
            "me encanta este servicio la atenci√≥n que recib√≠ fue maravillosa desde el primer momento me sent√≠ bien atendido el producto funciona perfectamente y cumple con todas mis expectativas",
        ]
        
        # Comentarios NEGATIVOS (palabras clave: mal, malo, p√©simo, insultos, etc.)
        negative_texts = [
            # Palabras clave simples negativas
            "mal", "malo", "mala", "p√©simo", "p√©sima", "terrible", "horrible",
            "basura", "ruin", "decepcionante", "decepcionado",
            # Insultos y expresiones negativas comunes
            "esta cagada", "es una mierda", "una porquer√≠a", "es basura",
            "no sirve", "no funciona", "no vale", "no recomiendo",
            # Frases comunes negativas
            "p√©simo servicio", "mal servicio", "muy mal servicio", "servicio p√©simo",
            "p√©simo producto", "mal producto", "muy mal producto", "producto p√©simo",
            "p√©sima atenci√≥n", "mal atenci√≥n", "muy mal atenci√≥n", "atenci√≥n p√©sima",
            "p√©sima calidad", "mal calidad", "muy mal calidad", "calidad p√©sima",
            # Frases completas negativas
            "p√©simo servicio muy malo", "no recomiendo para nada", "calidad terrible",
            "muy decepcionado", "atenci√≥n horrible", "lento e ineficiente", "no vale la pena",
            "muy insatisfecho", "problema grave", "no cumpli√≥ expectativas", "servicio p√©simo",
            "muy mala calidad", "no funciona bien", "muy decepcionante",
            "muy mal", "horrible experiencia", "p√©sima experiencia", "experiencia negativa",
            "no lo recomiendo", "no vale nada", "totalmente insatisfecho",
            "funciona mal", "no cumple expectativas", "muy por debajo de lo esperado",
            # Frases negativas con problemas t√©cnicos y de usabilidad (CR√çTICAS)
            "se cierra constantemente", "muy dif√≠cil de usar", "se cierra mucho",
            "aplicaci√≥n se cierra", "se cierra todo el tiempo", "constantemente se cierra",
            "muy dif√≠cil", "dif√≠cil de usar", "complicado de usar",
            "no responde", "se congela", "se queda congelada", "se bloquea",
            "no funciona", "no arranca", "no inicia", "no carga",
            "muy lento", "demasiado lento", "s√∫per lento", "extremadamente lento",
            "se queda colgado", "se cuelga", "se traba", "se detiene",
            "no sirve", "no funciona para nada", "no vale la pena",
            "problemas constantes", "muchos problemas", "siempre tiene problemas",
            "se cierra solo", "se cierra autom√°ticamente", "se cierra sin avisar",
            "muy complicado", "demasiado complicado", "complejo de usar",
            "servicio fue lento", "muy lento el servicio", "servicio lento",
            "qued√© insatisfecho", "muy insatisfecho", "totalmente insatisfecho",
            "no respondi√≥", "no responden", "no contestan", "no contestaron",
            "no respondi√≥ a mis mensajes", "no contestan mensajes", "no responden mensajes",
            "producto lleg√≥ da√±ado", "lleg√≥ da√±ado", "lleg√≥ roto", "lleg√≥ mal",
            "no cumpli√≥ expectativas", "no cumple expectativas", "no cumpli√≥",
            "mala experiencia", "horrible experiencia", "p√©sima experiencia",
            "no volver√© a comprar", "no comprar√© m√°s", "no recomiendo comprar",
            # P√°rrafos largos negativos (IMPORTANTE para detectar negatividad en p√°rrafos)
            "estoy muy decepcionado con este producto la calidad es terrible y el servicio al cliente fue p√©simo me tardaron mucho en responder y cuando lo hicieron no me ayudaron en nada el producto lleg√≥ da√±ado y no me quisieron dar reembolso no lo recomiendo para nada",
            "p√©sima experiencia de compra el producto no funciona como deber√≠a y el servicio al cliente es horrible me tardaron d√≠as en responder y cuando lo hicieron no me solucionaron nada el producto est√° defectuoso y no me quieren dar reembolso",
            "estoy muy insatisfecho con este servicio la atenci√≥n fue terrible desde el primer momento me sent√≠ mal atendido el producto no funciona bien y no cumple con mis expectativas no volver√© a comprar aqu√≠",
            "horrible experiencia el producto es de muy mala calidad y el servicio al cliente es p√©simo me ayudaron mal con mis dudas y el env√≠o tard√≥ mucho tiempo sin duda no volver√© a comprar aqu√≠",
            "estoy muy decepcionado con este producto la calidad es p√©sima y el servicio al cliente fue terrible me respondieron mal a todas mis preguntas y el producto lleg√≥ en malas condiciones no lo recomiendo",
            "muy mala experiencia el producto no funciona como deber√≠a y el servicio al cliente es horrible me tardaron mucho en responder y cuando lo hicieron no me solucionaron nada el producto est√° defectuoso",
            "p√©simo servicio la atenci√≥n que recib√≠ fue terrible desde el primer momento me sent√≠ mal atendido el producto funciona mal y no cumple con mis expectativas no volver√© a comprar aqu√≠",
            "estoy muy insatisfecho con este producto la calidad es terrible y el servicio al cliente fue p√©simo me ayudaron mal con todas mis dudas y el env√≠o tard√≥ mucho tiempo sin duda no lo recomiendo",
        ]
        
        # Comentarios NEUTRALES (palabras clave: normal, regular, aceptable, etc.)
        neutral_texts = [
            # Palabras clave simples neutrales
            "normal", "regular", "aceptable", "b√°sico", "est√°ndar", "com√∫n",
            "ni bueno ni malo", "ni mal ni bien", "sin m√°s", "nada especial",
            # Frases comunes neutrales
            "producto regular", "servicio regular", "atenci√≥n regular", "calidad regular",
            "producto normal", "servicio normal", "atenci√≥n normal", "calidad normal",
            "producto aceptable", "servicio aceptable", "atenci√≥n aceptable", "calidad aceptable",
            # Frases completas neutrales
            "ni bueno ni malo", "aceptable", "sin comentarios",
            "b√°sico", "est√°ndar", "cumple su funci√≥n", "nada especial", "producto com√∫n",
            "servicio est√°ndar", "normal como cualquier otro", "ni destacable ni malo",
            "producto promedio", "servicio b√°sico", "cumple con lo b√°sico",
            "ni destacable ni malo", "regular nada m√°s", "como se esperaba",
            "sin sorpresas", "ni bueno ni mal", "est√° bien",
            # P√°rrafos largos neutrales
            "el producto es normal cumple con su funci√≥n b√°sica pero no destaca en nada especial el servicio al cliente es regular y la calidad es aceptable sin m√°s comentarios",
            "experiencia regular el producto funciona como se espera pero no es nada especial el servicio al cliente es normal y la calidad es b√°sica cumple con lo b√°sico",
            "producto est√°ndar la calidad es normal y el servicio al cliente es aceptable no hay nada destacable pero tampoco hay problemas graves cumple con su funci√≥n",
        ]
        
        texts = positive_texts + negative_texts + neutral_texts
        labels = (['positivo'] * len(positive_texts) + 
                 ['negativo'] * len(negative_texts) + 
                 ['neutral'] * len(neutral_texts))
        
        print("üîÑ Entrenando red neuronal LSTM para comentarios de hasta 25 palabras...")
        print(f"üìä Total de textos: {len(texts)}, Clases: {len(set(labels))}")
        print(f"üîç [DEBUG] Textos positivos: {len(positive_texts)}, negativos: {len(negative_texts)}, neutrales: {len(neutral_texts)}")
        
        # Entrenamiento con m√°s √©pocas para mejor aprendizaje
        print("üîç [DEBUG] Iniciando entrenamiento...")
        try:
            # Entrenamiento ULTRA-R√ÅPIDO: 1 √©poca, batch_size autom√°tico (todas las muestras)
            history = self.train(texts, labels, epochs=1, batch_size=1000)  # 1 √©poca, batch grande (se ajustar√° autom√°ticamente)
            print("‚úÖ [DEBUG] M√©todo train() completado")
            
            # Validar que el modelo est√° entrenado
            print(f"üîç [DEBUG] Verificando estado del modelo despu√©s del entrenamiento...")
            print(f"üîç [DEBUG] is_trained: {self.is_trained}")
            print(f"üîç [DEBUG] model existe: {self.model is not None}")
            print(f"üîç [DEBUG] tokenizer tiene word_index: {hasattr(self.tokenizer, 'word_index') and self.tokenizer.word_index is not None}")
            print(f"üîç [DEBUG] label_encoder tiene classes: {hasattr(self.label_encoder, 'classes_') and len(self.label_encoder.classes_) > 0}")
            
            if not self.is_trained:
                raise ValueError("El modelo no se marc√≥ como entrenado despu√©s del entrenamiento")
            if not self.model:
                raise ValueError("El modelo no existe despu√©s del entrenamiento")
            
            print("üîç [DEBUG] Guardando modelo...")
            self.save_model()
            print("‚úÖ [DEBUG] Modelo guardado correctamente")
            
            # Validaci√≥n final: hacer una predicci√≥n de prueba
            print("üîç [DEBUG] Haciendo predicci√≥n de prueba despu√©s del entrenamiento...")
            test_result = self.predict_single("excelente servicio")
            print(f"‚úÖ [DEBUG] Predicci√≥n de prueba exitosa: sentiment={test_result.get('sentiment')}, score={test_result.get('score')}")
            
        except Exception as e:
            print(f"‚ùå [DEBUG] Error en _create_pretrained_model: {str(e)}")
            import traceback
            traceback.print_exc()
            self.is_trained = False
            raise
        
        print("‚úÖ Red neuronal LSTM entrenada correctamente (soporta comentarios de hasta 25 palabras)")
    
    def save_model(self, model_path: str = 'app/ml_models/sentiment_model.keras'):
        """Guardar modelo en formato .keras (compatible con Keras 3.x)"""
        model_dir = os.path.dirname(model_path)
        os.makedirs(model_dir, exist_ok=True)
        
        if self.model:
            # Guardar en formato .keras (m√°s compatible con Keras 3.x)
            # Keras 3.x infiere autom√°ticamente el formato desde la extensi√≥n .keras
            self.model.save(model_path)
            print(f"‚úÖ Modelo guardado en formato .keras: {model_path}")
        
        tokenizer_path = os.path.join(model_dir, 'tokenizer.pkl')
        label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
        
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(self.tokenizer, f)
        with open(label_encoder_path, 'wb') as f:
            pickle.dump(self.label_encoder, f)
        
        print(f"‚úÖ Tokenizer guardado en: {tokenizer_path}")
        print(f"‚úÖ Label encoder guardado en: {label_encoder_path}")

